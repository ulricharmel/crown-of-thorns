{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This preliminary notebook using some code form https://www.kaggle.com/mrinath/efficientdet-train-pytorch","metadata":{}},{"cell_type":"code","source":"# Try doing all the installations here\n!pip install -I numpy\n\n!pip install -I torchvision\n!pip install -I torch -U   \n\n# First, we need to install pycocotools. This library will be used for computing the evaluation metrics following the COCO metric for intersection over union.\n\n!pip install cython\n# Install pycocotools, the version by default in Colab\n# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n!pip install -I 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI' --no-binary pycocotools\n!ls /kaggle/input/baseline-predict-pytorch\n\n# # !pip install -I pycocotools==2.0.0\n","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:31:03.623303Z","iopub.execute_input":"2022-02-05T00:31:03.623623Z","iopub.status.idle":"2022-02-05T00:33:47.816692Z","shell.execute_reply.started":"2022-02-05T00:31:03.623538Z","shell.execute_reply":"2022-02-05T00:33:47.815719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !ls /kaggle/input/baseline-predict-pytorch","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:33:47.818676Z","iopub.execute_input":"2022-02-05T00:33:47.818937Z","iopub.status.idle":"2022-02-05T00:33:47.824098Z","shell.execute_reply.started":"2022-02-05T00:33:47.818883Z","shell.execute_reply":"2022-02-05T00:33:47.823345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !cp -r /kaggle/input/cococode/PythonAPI/pycocotools/ /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:33:47.851212Z","iopub.execute_input":"2022-02-05T00:33:47.851958Z","iopub.status.idle":"2022-02-05T00:33:47.85559Z","shell.execute_reply.started":"2022-02-05T00:33:47.85192Z","shell.execute_reply":"2022-02-05T00:33:47.854793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !python /kaggle/input/cococode/PythonAPI/setup.py  build_ext install","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:33:47.857242Z","iopub.execute_input":"2022-02-05T00:33:47.858043Z","iopub.status.idle":"2022-02-05T00:33:47.863944Z","shell.execute_reply.started":"2022-02-05T00:33:47.858003Z","shell.execute_reply":"2022-02-05T00:33:47.863086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pycocotools._mask as mask","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:33:47.865694Z","iopub.execute_input":"2022-02-05T00:33:47.866368Z","iopub.status.idle":"2022-02-05T00:33:47.873923Z","shell.execute_reply.started":"2022-02-05T00:33:47.866331Z","shell.execute_reply":"2022-02-05T00:33:47.873077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# git clone the utility functions and evaluation functions from pytorch coco dataset\n\n!git clone https://github.com/pytorch/vision.git\n!cd vision\n!git checkout v0.8.2","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:33:47.875171Z","iopub.execute_input":"2022-02-05T00:33:47.875564Z","iopub.status.idle":"2022-02-05T00:34:08.351708Z","shell.execute_reply.started":"2022-02-05T00:33:47.875528Z","shell.execute_reply":"2022-02-05T00:34:08.350904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls vision/references","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:08.353661Z","iopub.execute_input":"2022-02-05T00:34:08.354208Z","iopub.status.idle":"2022-02-05T00:34:09.071776Z","shell.execute_reply.started":"2022-02-05T00:34:08.354161Z","shell.execute_reply":"2022-02-05T00:34:09.070944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp vision/references/detection/utils.py /kaggle/working/\n!cp vision/references/detection/transforms.py /kaggle/working/\n!cp vision/references/detection/coco_eval.py /kaggle/working/\n!cp vision/references/detection/engine.py /kaggle/working/\n!cp vision/references/detection/coco_utils.py /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:09.07315Z","iopub.execute_input":"2022-02-05T00:34:09.074919Z","iopub.status.idle":"2022-02-05T00:34:12.423407Z","shell.execute_reply.started":"2022-02-05T00:34:09.074861Z","shell.execute_reply":"2022-02-05T00:34:12.422518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport ast\nimport sys\nimport os\n\n# os.environ['TORCH_HOME'] = '\\\\kaggle\\\\input\\\\resnet'\n\nimport glob\nimport sklearn\nimport math\nimport random\n\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom transformers import get_cosine_schedule_with_warmup\n\nfrom PIL import Image\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn import metrics, model_selection, preprocessing\nfrom sklearn.model_selection import GroupKFold\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(device)\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-02-05T00:34:12.425044Z","iopub.execute_input":"2022-02-05T00:34:12.425474Z","iopub.status.idle":"2022-02-05T00:34:19.070344Z","shell.execute_reply.started":"2022-02-05T00:34:12.42543Z","shell.execute_reply":"2022-02-05T00:34:19.069572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train only on images with detections let see\ndf = pd.read_csv(\"../input/tensorflow-great-barrier-reef/train.csv\")\n# df = df[df.annotations != '[]']\n# df = df.reset_index(drop = True)\ndf.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:19.071539Z","iopub.execute_input":"2022-02-05T00:34:19.072257Z","iopub.status.idle":"2022-02-05T00:34:19.160265Z","shell.execute_reply.started":"2022-02-05T00:34:19.072216Z","shell.execute_reply":"2022-02-05T00:34:19.159564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['fold'] = -1\nkf = GroupKFold(n_splits = 5)\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df, y = df.video_id.tolist(), groups=df.sequence)):\n    df.loc[val_idx, 'fold'] = fold","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:19.161383Z","iopub.execute_input":"2022-02-05T00:34:19.162074Z","iopub.status.idle":"2022-02-05T00:34:19.181494Z","shell.execute_reply.started":"2022-02-05T00:34:19.162032Z","shell.execute_reply":"2022-02-05T00:34:19.180813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:19.182521Z","iopub.execute_input":"2022-02-05T00:34:19.182922Z","iopub.status.idle":"2022-02-05T00:34:19.195664Z","shell.execute_reply.started":"2022-02-05T00:34:19.182871Z","shell.execute_reply":"2022-02-05T00:34:19.194889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.fold.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:19.197145Z","iopub.execute_input":"2022-02-05T00:34:19.197708Z","iopub.status.idle":"2022-02-05T00:34:19.210512Z","shell.execute_reply.started":"2022-02-05T00:34:19.197666Z","shell.execute_reply":"2022-02-05T00:34:19.209731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add the imaging paths to the dataframe\ndf['path'] = [f\"../input/tensorflow-great-barrier-reef/train_images/video_{a}/{b}.jpg\" for a,b in zip(df[\"video_id\"],df[\"video_frame\"])]\ndf['annotations'] = df['annotations'].apply(eval)\ndf['number_boxes'] = df['annotations'].apply(lambda x: len(x))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:19.211913Z","iopub.execute_input":"2022-02-05T00:34:19.212376Z","iopub.status.idle":"2022-02-05T00:34:19.644332Z","shell.execute_reply.started":"2022-02-05T00:34:19.212341Z","shell.execute_reply":"2022-02-05T00:34:19.643675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot some of the images\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches\n\ndef get_rectangle_edges_from_pascal_bbox(bbox):\n    xmin_top_left, ymin_top_left, xmax_bottom_right, ymax_bottom_right = bbox\n\n    bottom_left = (xmin_top_left, ymax_bottom_right)\n    width = xmax_bottom_right - xmin_top_left\n    height = ymin_top_left - ymax_bottom_right\n\n    return bottom_left, width, height\n\ndef draw_pascal_voc_bboxes(\n    plot_ax,\n    bboxes,\n    get_rectangle_corners_fn=get_rectangle_edges_from_pascal_bbox,\n):\n    for bbox in bboxes:\n        bottom_left, width, height = get_rectangle_corners_fn(bbox)\n\n        rect_1 = patches.Rectangle(\n            bottom_left,\n            width,\n            height,\n            linewidth=2,\n            edgecolor=\"black\",\n            fill=False,\n        )\n        rect_2 = patches.Rectangle(\n            bottom_left,\n            width,\n            height,\n            linewidth=2,\n            edgecolor=\"red\",\n            fill=False,\n        )\n\n        # Add the patch to the Axes\n        plot_ax.add_patch(rect_1)\n        plot_ax.add_patch(rect_2)\n\ndef draw_image(\n    image, bboxes=None, draw_bboxes_fn=draw_pascal_voc_bboxes, figsize=(10, 10)\n):\n    fig, ax = plt.subplots(1, figsize=figsize)\n    ax.imshow(image)\n\n    if bboxes is not None:\n        draw_bboxes_fn(ax, bboxes)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:19.645461Z","iopub.execute_input":"2022-02-05T00:34:19.646159Z","iopub.status.idle":"2022-02-05T00:34:19.657493Z","shell.execute_reply.started":"2022-02-05T00:34:19.646121Z","shell.execute_reply":"2022-02-05T00:34:19.656651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataAdaptor:\n    def __init__(self,df):\n        self.df = df\n    def __len__(self):\n        return len(self.df)\n    \n    def get_boxes(self, row):\n        \"\"\"Returns the bboxes for a given row as a 3D matrix with format [x_min, y_min, x_max, y_max]\"\"\"\n        \n        boxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\n        \n        # Change from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n        boxes[:, 2] = np.clip(boxes[:, 0] + boxes[:, 2],0,1280)\n        boxes[:, 3] = np.clip(boxes[:, 1] + boxes[:, 3],0,720) \n        \n        return boxes\n    \n    def get_image_bb(self , idx):\n        img_src = self.df.loc[idx,'path']\n        image   = cv2.imread(img_src)\n        image   = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        row     = self.df.iloc[idx]\n        bboxes  = self.get_boxes(row) \n        class_labels = np.ones(len(bboxes))\n        return image, bboxes, class_labels, idx\n    \n        \n    def show_image(self, index):\n        image, bboxes, class_labels, image_id = self.get_image_bb(index)\n        print(f\"image_id: {image_id}\")\n        draw_image(image, bboxes.tolist())\n#         print(class_labels) \n        return image","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:19.659244Z","iopub.execute_input":"2022-02-05T00:34:19.659626Z","iopub.status.idle":"2022-02-05T00:34:19.673975Z","shell.execute_reply.started":"2022-02-05T00:34:19.65959Z","shell.execute_reply":"2022-02-05T00:34:19.673191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = DataAdaptor(df)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:19.675449Z","iopub.execute_input":"2022-02-05T00:34:19.675702Z","iopub.status.idle":"2022-02-05T00:34:19.686725Z","shell.execute_reply.started":"2022-02-05T00:34:19.675668Z","shell.execute_reply":"2022-02-05T00:34:19.685746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im,bb,_,_ = train_ds.get_image_bb(4005)\nbb","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:19.688083Z","iopub.execute_input":"2022-02-05T00:34:19.688758Z","iopub.status.idle":"2022-02-05T00:34:19.766754Z","shell.execute_reply.started":"2022-02-05T00:34:19.688719Z","shell.execute_reply":"2022-02-05T00:34:19.766129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = train_ds.show_image(2016)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:19.767865Z","iopub.execute_input":"2022-02-05T00:34:19.768231Z","iopub.status.idle":"2022-02-05T00:34:20.431328Z","shell.execute_reply.started":"2022-02-05T00:34:19.768198Z","shell.execute_reply":"2022-02-05T00:34:20.430576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.where(df[\"number_boxes\"] > 2)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:20.433Z","iopub.execute_input":"2022-02-05T00:34:20.433687Z","iopub.status.idle":"2022-02-05T00:34:20.441159Z","shell.execute_reply.started":"2022-02-05T00:34:20.433647Z","shell.execute_reply":"2022-02-05T00:34:20.440373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_seq = [len(df[df['video_id'] == i]) for i in range(3)]\nlabels = [\"0\", \"1\", \"2\"]\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(9,6))\nax.set_facecolor('aliceblue')\nplt.grid(color=\"gray\", linestyle=\"-\", zorder=0)\nplt.ylabel(\"Number of Frames\", fontsize=16, fontweight=\"bold\")\nplt.xlabel(\"Video ID\", fontsize=16, fontweight=\"bold\")\nplt.title(\"Length of train videos\", fontsize=20, fontweight=\"bold\")\nplt.bar(labels, num_seq, color=\"orange\", zorder=3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:20.447832Z","iopub.execute_input":"2022-02-05T00:34:20.448459Z","iopub.status.idle":"2022-02-05T00:34:20.661461Z","shell.execute_reply.started":"2022-02-05T00:34:20.448425Z","shell.execute_reply":"2022-02-05T00:34:20.660802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_num = max(df.number_boxes)\nmax_sample = df[df[\"number_boxes\"] == max_num].sample()\nmax_vid_id = max_sample.video_id.values[0]\nmax_vid_frame = max_sample.video_frame.values[0]\n\nprint('\\033[1m' + f\"Maximum number of starfish in one frame: {max_num} (Video {max_vid_id}, Frame {max_vid_frame})\" + '\\033[0m')","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:20.662728Z","iopub.execute_input":"2022-02-05T00:34:20.663122Z","iopub.status.idle":"2022-02-05T00:34:20.680996Z","shell.execute_reply.started":"2022-02-05T00:34:20.663084Z","shell.execute_reply":"2022-02-05T00:34:20.680148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = train_ds.show_image(max_sample.index[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:20.682379Z","iopub.execute_input":"2022-02-05T00:34:20.68282Z","iopub.status.idle":"2022-02-05T00:34:21.151241Z","shell.execute_reply.started":"2022-02-05T00:34:20.682784Z","shell.execute_reply":"2022-02-05T00:34:21.150533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check number of samples without boxes\nmin_num = 0\nmin_sample = df[df[\"number_boxes\"] == 0]\nprint(len(min_sample), len(df), len(df)-len(min_sample))","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:21.152829Z","iopub.execute_input":"2022-02-05T00:34:21.153196Z","iopub.status.idle":"2022-02-05T00:34:21.162443Z","shell.execute_reply.started":"2022-02-05T00:34:21.153163Z","shell.execute_reply":"2022-02-05T00:34:21.161646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training an Object Detection Model\nBefore we settled on using pytorch, we trained the model  using Tensorflow and Keras libraries but Pytorch performed better. \nWe used the FasterR-CNN pre-trained model and fine-tuned it as per the requirements of this project.\nPyTorch uses torch.utils.data.DataLoader and torch.utils.data.Dataset class to work with data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset. \nFor this object detection project, we will be using a TorchVision dataset. To declare, initialize, and manipulate objects in Python, we use classes. \nThe __getitem__ reads the image using the image_id we have in the dataframe, and also we can get all the bounding boxes associated with that image. We then initialize a dict called target, which will be passed to model for training. This target will have metadata of the annotation like actual bounding box coordinates, it’s corresponding labels, image_id, area of the bounding boxes. The area parameter is used during evaluation with the COCO metric, to separate the metric scores between small, medium, and large boxes. If we set iscrowd as True, those instances will be ignored during evaluation. The __len__ method gives the size of the Dataset.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.utils.data\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch import optim\nfrom torchvision import transforms\n\n\nclass CotsData(torch.utils.data.Dataset):\n    def __init__(self, df, transforms=None):\n        self.ds = df\n        self.transforms = transforms\n    \n    def get_boxes(self, row):\n        \"\"\"Returns the bboxes for a given row as a 3D matrix with format [x_min, y_min, x_max, y_max]\"\"\"\n        \n        boxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\n        \n        # Change from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n        boxes[:, 2] = np.clip(boxes[:, 0] + boxes[:, 2],0,1280)\n        boxes[:, 3] = np.clip(boxes[:, 1] + boxes[:, 3],0,720) \n        \n        return boxes\n            \n    def __getitem__(self, idx):\n        # load images\n        img_path = self.ds.loc[idx,'path']\n        # mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        # note that we haven't converted the mask to RGB,\n        \n        row = self.ds.iloc[idx]\n        boxes = self.get_boxes(row)\n        num_objs = self.ds.loc[idx, 'number_boxes']\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        \n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64) # check this probably have to set this to true\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.ds)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:21.16395Z","iopub.execute_input":"2022-02-05T00:34:21.164369Z","iopub.status.idle":"2022-02-05T00:34:21.182535Z","shell.execute_reply.started":"2022-02-05T00:34:21.164329Z","shell.execute_reply":"2022-02-05T00:34:21.181614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input/tensorflow-great-barrier-reef","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:21.198835Z","iopub.execute_input":"2022-02-05T00:34:21.199122Z","iopub.status.idle":"2022-02-05T00:34:21.904015Z","shell.execute_reply.started":"2022-02-05T00:34:21.19905Z","shell.execute_reply":"2022-02-05T00:34:21.903222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_npy = np.load(\"/kaggle/input/tensorflow-great-barrier-reef/example_test.npy\")","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:21.905483Z","iopub.execute_input":"2022-02-05T00:34:21.905699Z","iopub.status.idle":"2022-02-05T00:34:22.241918Z","shell.execute_reply.started":"2022-02-05T00:34:21.905671Z","shell.execute_reply":"2022-02-05T00:34:22.24116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testdf = pd.read_csv(\"/kaggle/input/tensorflow-great-barrier-reef/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:22.24357Z","iopub.execute_input":"2022-02-05T00:34:22.24385Z","iopub.status.idle":"2022-02-05T00:34:22.256846Z","shell.execute_reply.started":"2022-02-05T00:34:22.243804Z","shell.execute_reply":"2022-02-05T00:34:22.256224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subdf = pd.read_csv(\"/kaggle/input/tensorflow-great-barrier-reef/example_sample_submission.csv\")\nsubdf","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:22.258277Z","iopub.execute_input":"2022-02-05T00:34:22.259118Z","iopub.status.idle":"2022-02-05T00:34:22.275012Z","shell.execute_reply.started":"2022-02-05T00:34:22.259061Z","shell.execute_reply":"2022-02-05T00:34:22.27437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat transforms.py","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:22.275963Z","iopub.execute_input":"2022-02-05T00:34:22.276593Z","iopub.status.idle":"2022-02-05T00:34:22.956251Z","shell.execute_reply.started":"2022-02-05T00:34:22.276558Z","shell.execute_reply":"2022-02-05T00:34:22.955452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input/tensorflow-great-barrier-reef/greatbarrierreef/","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:22.95837Z","iopub.execute_input":"2022-02-05T00:34:22.958689Z","iopub.status.idle":"2022-02-05T00:34:23.638939Z","shell.execute_reply.started":"2022-02-05T00:34:22.958648Z","shell.execute_reply":"2022-02-05T00:34:23.638055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training, Validation Datasets and Data Loaders.\nThe next step is to create our training and validation datasets. The data loader loads the training data in batches into the model for training. For this also we will be using PyTorch’s DataLoader utility.","metadata":{}},{"cell_type":"code","source":"# get training and validation dataframes\ndef get_train_val(df, train=True):\n    if train:\n        df2 = df[df.video_id != 2]\n        dfn = df2[df.number_boxes>0]\n        dfo = df2[df.number_boxes==0]\n        dfno = dfo.sample(n=1000, replace=False, random_state=1)\n        result = pd.concat([dfn, dfno])\n    else:\n        df2 = df[df.video_id==2]\n        dfn = df2[df.number_boxes>0]\n        dfo = df2[df.number_boxes==0]\n        dfno = dfo.sample(n=100, replace=False, random_state=1)\n        result = pd.concat([dfn, dfno])\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:23.668308Z","iopub.execute_input":"2022-02-05T00:34:23.668555Z","iopub.status.idle":"2022-02-05T00:34:23.677359Z","shell.execute_reply.started":"2022-02-05T00:34:23.668521Z","shell.execute_reply":"2022-02-05T00:34:23.676574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fold_n = 1\n# train_df= df[df.fold != fold_n]\n# val_df  = df[df.fold == fold_n]\n\ntrain_df = get_train_val(df, train=True)\nval_df = get_train_val(df, train=False)\n\n# use our dataset and defined transformations\ndataset = CotsData(train_df.reset_index(drop=True), get_transform(train=True))\ndataset_test = CotsData(val_df.reset_index(drop=True), get_transform(train=False))","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:23.679273Z","iopub.execute_input":"2022-02-05T00:34:23.67994Z","iopub.status.idle":"2022-02-05T00:34:23.714078Z","shell.execute_reply.started":"2022-02-05T00:34:23.679886Z","shell.execute_reply":"2022-02-05T00:34:23.71344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the dataset in train and test set\ntorch.manual_seed(1)\n# indices = torch.randperm(len(dataset)).tolist()\n# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=8, shuffle=True, num_workers=4,\n    collate_fn=utils.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n    collate_fn=utils.collate_fn)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:23.715519Z","iopub.execute_input":"2022-02-05T00:34:23.71576Z","iopub.status.idle":"2022-02-05T00:34:23.72404Z","shell.execute_reply.started":"2022-02-05T00:34:23.715726Z","shell.execute_reply":"2022-02-05T00:34:23.723223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from engine import train_one_epoch, evaluate\nimport utils\nimport transforms as T\n\n\ndef get_transform(train):\n    transforms = []\n    # converts the image, a PIL image, into a PyTorch Tensor\n    transforms.append(T.ToTensor())\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        transforms.append(T.RandomHorizontalFlip(0.5))\n#         transforms.append(T.RandomPhotometricDistort())\n#         transforms.append(T.RandomZoomOut())\n    return T.Compose(transforms)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:23.640457Z","iopub.execute_input":"2022-02-05T00:34:23.640671Z","iopub.status.idle":"2022-02-05T00:34:23.666845Z","shell.execute_reply.started":"2022-02-05T00:34:23.640641Z","shell.execute_reply":"2022-02-05T00:34:23.666212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Faster R-CNN\nThe Faster R-CNN profoundly replaces the Selective Search technique with much efficient Region Proposal Network that generates the detected areas in an image. It is highly used for carrying out real-time performance-based tasks in object detection tasks. \nWe initialize our model using torchvision’s FasterRCNN with a resnet50 backbone. \nWe set pretrained as true, so the function will return a model pre-trained on COCO. Here we set the num_classes as 2, considering background as one class.\nBefore we start the training we can declare the number of epochs to train and also set the optimizer and learning rate scheduler.\nThe optimizer we are using here is SGD (Stochastic Gradient Descent). The learning rate scheduler helps to adjust the learning rate during the course of the training to achieve more accuracy and speed up convergence. We use StepLR scheduler which decays the learning rate of each parameter group by gamma every step_size epochs. The gamma and step_size hyperparameters will decide the learning rate decay. Finally, we gonna train this model for 20 epochs.","metadata":{}},{"cell_type":"code","source":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n      \ndef get_instance_segmentation_model(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) # trainable_backbone_layers=4\n    #model.load_state_dict(torch.load('/kaggle/input/resnet/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth'))\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    #hidden_layer = 256\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:21.184043Z","iopub.execute_input":"2022-02-05T00:34:21.184465Z","iopub.status.idle":"2022-02-05T00:34:21.196779Z","shell.execute_reply.started":"2022-02-05T00:34:21.184427Z","shell.execute_reply":"2022-02-05T00:34:21.195989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n\n# get the model using our helper function\nmodel = get_instance_segmentation_model(num_classes)\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.05,\n                            momentum=0.9, weight_decay=0.0005)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:23.725685Z","iopub.execute_input":"2022-02-05T00:34:23.726313Z","iopub.status.idle":"2022-02-05T00:34:28.133985Z","shell.execute_reply.started":"2022-02-05T00:34:23.726277Z","shell.execute_reply":"2022-02-05T00:34:28.133231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.optim.lr_scheduler import StepLR\nnum_epochs = 20\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, data_loader_test, device=device)\n\ntorch.save(model.state_dict(), 'checkpoint-video2.pth')","metadata":{"execution":{"iopub.status.busy":"2022-02-05T00:34:28.135119Z","iopub.execute_input":"2022-02-05T00:34:28.135368Z","iopub.status.idle":"2022-02-05T09:24:49.772545Z","shell.execute_reply.started":"2022-02-05T00:34:28.135336Z","shell.execute_reply":"2022-02-05T09:24:49.771648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model is saved for inference to make our predictions. Once the training is completed we will have the model.pth file.","metadata":{}},{"cell_type":"markdown","source":"# Observations During Training\nDuring the fine tuning process of our model, we included a trainable backbone layers of 4 and trained for 30 epochs which caused our model to perfom poorly most likely due to Over fitting.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PATH = 'checkpoint.pth'\n# torch.save({\n#             'epoch': epoch,\n#             'model_state_dict': model.state_dict(),\n#             'optimizer_state_dict': optimizer.state_dict(),\n#             'loss': loss,\n#             }, PATH)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-05T09:24:49.77437Z","iopub.execute_input":"2022-02-05T09:24:49.774926Z","iopub.status.idle":"2022-02-05T09:24:49.779231Z","shell.execute_reply.started":"2022-02-05T09:24:49.774867Z","shell.execute_reply":"2022-02-05T09:24:49.778427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir(r'../working')\nfrom IPython.display import FileLink\nFileLink(r'checkpoint-video2.pth')","metadata":{"execution":{"iopub.status.busy":"2022-02-05T09:24:49.780293Z","iopub.execute_input":"2022-02-05T09:24:49.78099Z","iopub.status.idle":"2022-02-05T09:24:49.796606Z","shell.execute_reply.started":"2022-02-05T09:24:49.780954Z","shell.execute_reply":"2022-02-05T09:24:49.795862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_path = '/kaggle/input/savemodel/checkpoint.pth'","metadata":{"execution":{"iopub.status.busy":"2022-02-05T09:24:49.79783Z","iopub.execute_input":"2022-02-05T09:24:49.798637Z","iopub.status.idle":"2022-02-05T09:24:49.802217Z","shell.execute_reply.started":"2022-02-05T09:24:49.798531Z","shell.execute_reply":"2022-02-05T09:24:49.80147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# state_dict = torch.load(model_path)\n# # print(state_dict.keys())\n# model.load_state_dict(state_dict)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T09:24:49.803381Z","iopub.execute_input":"2022-02-05T09:24:49.804242Z","iopub.status.idle":"2022-02-05T09:24:49.810637Z","shell.execute_reply.started":"2022-02-05T09:24:49.804205Z","shell.execute_reply":"2022-02-05T09:24:49.809908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def apply_nms(orig_prediction, iou_thresh=0.3, score_thresh=0.35):\n    \n#     # torchvision returns the indices of the bboxes to keep\n#     # function to implement non maximm suppression\n#     # might also need to eliminate predictions with very low scores\n#     # trim low scores first\n    \n#     keep = orig_prediction['scores'] >= score_thresh\n    \n#     scores_prediction = {}\n#     scores_prediction['boxes'] = orig_prediction['boxes'][keep]\n#     scores_prediction['scores'] = orig_prediction['scores'][keep]\n#     scores_prediction['labels'] = orig_prediction['labels'][keep]\n    \n#     keep = torchvision.ops.nms(scores_prediction['boxes'], scores_prediction['scores'], iou_thresh)\n    \n#     final_prediction = {}\n#     final_prediction['boxes'] = scores_prediction['boxes'][keep]\n#     final_prediction['scores'] = scores_prediction['scores'][keep]\n#     final_prediction['labels'] = scores_prediction['labels'][keep]\n    \n#     return final_prediction\n\n# def return_predict_string(predictions):\n#     str_p = ''\n#     for i, score in enumerate(predictions['scores']):\n#         box = predictions['boxes'][i].cpu()\n#         str_p += f'{score} {int(np.round(box[0]))} {int(np.round(box[1]))} {int(np.round(box[2]-box[0]))} {int(np.round(box[3]-box[1]))} '\n    \n#     str_p = str_p.strip(' ')\n#     if str_p == '':\n#         str_p = '0.9 716 678 54 42'\n    \n#     return str_p\n\n# def preprocess_img(img):\n#     img = img/255.\n#     x,y, c = img.shape\n#     img = img.reshape(c,x,y)\n#     return torch.from_numpy(img)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T09:24:49.811979Z","iopub.execute_input":"2022-02-05T09:24:49.812564Z","iopub.status.idle":"2022-02-05T09:24:49.819211Z","shell.execute_reply.started":"2022-02-05T09:24:49.812527Z","shell.execute_reply":"2022-02-05T09:24:49.818454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # pick one image from the test set\n# img, target = dataset_test[5]\n# # put the model in evaluation mode\n# model.eval()\n# with torch.no_grad():\n#     prediction = model([img.to(device=device, dtype=torch.float)])[0]\n#     final_pred = apply_nms(prediction, 0.2)\n    \n# print('predicted #boxes: ', len(prediction['labels']))\n# print('real #boxes: ', len(target['labels']))\n# print('nms predict #boxes: ', len(final_pred['labels']))\n# print('scores: ', prediction['scores'])\n# print(return_predict_string(prediction))\n\n# print(prediction)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T09:24:49.820504Z","iopub.execute_input":"2022-02-05T09:24:49.82103Z","iopub.status.idle":"2022-02-05T09:24:49.830804Z","shell.execute_reply.started":"2022-02-05T09:24:49.820995Z","shell.execute_reply":"2022-02-05T09:24:49.830062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import greatbarrierreef\n# rows=[]\n# ii = 0\n# env = greatbarrierreef.make_env()   # initialize the environment\n# iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\n# for (pixel_array, sample_prediction_df) in iter_test:\n#     pixel_p = preprocess_img(pixel_array)\n#     prediction = model([pixel_p.to(device, dtype=torch.float)])[0]\n#     sample_prediction_df['annotations'] = anno = '0.5 0 0 100 100' #return_predict_string(apply_nms(prediction, 0.3))  # make your predictions here\n#     rows.append([ii, anno])\n#     env.predict(sample_prediction_df)\n#     ii += 1\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-05T09:24:49.832259Z","iopub.execute_input":"2022-02-05T09:24:49.832672Z","iopub.status.idle":"2022-02-05T09:24:49.842377Z","shell.execute_reply.started":"2022-02-05T09:24:49.832636Z","shell.execute_reply":"2022-02-05T09:24:49.84146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rows\n# model([pixel_p.to(device=device, dtype=torch.float)])[0]","metadata":{"execution":{"iopub.status.busy":"2022-02-05T09:24:49.844073Z","iopub.execute_input":"2022-02-05T09:24:49.844406Z","iopub.status.idle":"2022-02-05T09:24:49.850211Z","shell.execute_reply.started":"2022-02-05T09:24:49.844374Z","shell.execute_reply":"2022-02-05T09:24:49.849412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_ds = DataAdaptor(val_df.reset_index(drop=True))","metadata":{"execution":{"iopub.status.busy":"2022-02-05T09:24:49.852223Z","iopub.execute_input":"2022-02-05T09:24:49.852487Z","iopub.status.idle":"2022-02-05T09:24:49.857812Z","shell.execute_reply.started":"2022-02-05T09:24:49.852453Z","shell.execute_reply":"2022-02-05T09:24:49.857047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# img5 = test_ds.show_image(5)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T09:24:49.859691Z","iopub.execute_input":"2022-02-05T09:24:49.859961Z","iopub.status.idle":"2022-02-05T09:24:49.866577Z","shell.execute_reply.started":"2022-02-05T09:24:49.859927Z","shell.execute_reply":"2022-02-05T09:24:49.865733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pixel_p*255.","metadata":{"execution":{"iopub.status.busy":"2022-02-05T09:24:49.868566Z","iopub.execute_input":"2022-02-05T09:24:49.869144Z","iopub.status.idle":"2022-02-05T09:24:49.874321Z","shell.execute_reply.started":"2022-02-05T09:24:49.869108Z","shell.execute_reply":"2022-02-05T09:24:49.873548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.imshow(pixel_p.reshape(720,1280,3))","metadata":{"execution":{"iopub.status.busy":"2022-02-05T09:24:49.875621Z","iopub.execute_input":"2022-02-05T09:24:49.876524Z","iopub.status.idle":"2022-02-05T09:24:49.882223Z","shell.execute_reply.started":"2022-02-05T09:24:49.876451Z","shell.execute_reply":"2022-02-05T09:24:49.881482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}