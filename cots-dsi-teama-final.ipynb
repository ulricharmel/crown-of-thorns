{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle Competition: TensorFlow - Help Protect the Great Barrier Reef","metadata":{"execution":{"iopub.status.busy":"2022-02-09T05:24:36.414824Z","iopub.execute_input":"2022-02-09T05:24:36.415287Z","iopub.status.idle":"2022-02-09T05:24:39.871754Z","shell.execute_reply.started":"2022-02-09T05:24:36.415226Z","shell.execute_reply":"2022-02-09T05:24:39.87078Z"}}},{"cell_type":"markdown","source":"## Goal:\nAccurately identify crown-of-thorns starfish (COTS) by building an object detection model trained on underwater videos of coral reefs.\n\n\n## Context:\nThe Great Barrier Reef is the worldâ€™s largest coral reef, home to over 1000 species of fishes, 400 species of corals, 130 species of sharks, rays, and a massive variety of other sea life. Unfortunately, the reef is under threat, in part because of the overpopulation of COTS. Hence, a consortium of scientists, tourism operators and reef managers established an intervention program to control COTS outbreaks, whereby a Kaggle competition has been setup to help towards this goal.\n\nTraditionally, the \"Manta Tow\" performed by a snorkel diver being towed by a boat is employed as a reef survey method. Every 200m, the diver would pause to record variables and visually assess the reef. The main drawbacks of this method are operational scalability, data resolution, reliability, and traceability. Through the Great Barrier Reef Foundation, a new survey and intervention method is being developed which will utilise underwater cameras to collect thousands of reef images, and AI algorithms to improve the efficiency and scale at which COTS are being detected to better mitigate their outbreaks.\n\nThrough this Kaggle crowd-sourcing competition, it is expected that an innovative machine/deep learning algorithm would emerge that can analyse large image datasets accurately, efficiently, and in near real-time.\n\n## Team members\nMarcelina, Nancy, Ulrich, Akhil","metadata":{}},{"cell_type":"markdown","source":"## Assessment metric\n\nThis Kaggle competition is evaluated on the [F2 Score at different intersection over union (IoU) thresholds](https://www.kaggle.com/c/tensorflow-great-barrier-reef/overview/evaluation). An F2 metric has been selected for this particular case since it weights recall more heavily than precision, given that it would be more beneficial to ensure very few starfish are missed by tolerating some false positives.\n\nPrecision looks at how accurately the true positives have been identified from the total retrieved elements, whereby keeping false positives to a minimum is preferred. On the other hand, recall looks at how accurately true positives have been identified from the total of relevant elements consisting of true positives and false negatives, that is the total number of elements that should have been identified correctly as positive. \n\n\nThe F2 score is computed using the equation below, where beta is replaced by 2.\n\n<img src= \"attachment:428d4d0c-efc6-4239-8794-c6529a90dab7.jpg\" style='width: 400px;'>\n\n\nImage courtesy of Wikipedia: [F-score](https://en.wikipedia.org/wiki/F-score)\n\n\nAs a side note, the IoU is a method for assessing the overlap between two bounding boxes, usually the ground-truth versus the prediction bounding boxes. The mathematical expression of IoU computation is given in the image below, where an IoU = 1 exists when the prediction is completely correct. Thus, the lower the IoU score, the worse the prediction.\n\n<img src= \"attachment:dae1a811-eb44-4d6f-bbe3-2749611c989d.png\" style='width: 400px;'>\n\nImage courtesy of Wikipedia: [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index)\n","metadata":{},"attachments":{"428d4d0c-efc6-4239-8794-c6529a90dab7.jpg":{"image/jpeg":"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"},"dae1a811-eb44-4d6f-bbe3-2749611c989d.png":{"image/png":"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"}}},{"cell_type":"markdown","source":"## Methodology\n\nOur initial approach was for every team member to choose his preferred framework, and to try to build up a baseline model. After comparing the initial results from each of the baseline models, we proceeded to fine-tune and improve the model with the inital best score. Only one of the team members had previous experience with deep learning techniques, he went ahead training the model with his preferred framework, that is PyTorch. The other three members did not have any previous experience with deep learning nor computer vision, hence initially had to gained a foundational knowledge of those. They explored Keras and Tensorflow but ultimately these were dropped since we could not get these notebooks to properly submit to Kaggle. Thus, we decided to go with PyTorch. \n\nGiven how complicated it is to trained an object detection network from scratch and limited time and computing resources available for the competition, we adopted the strategy of fine-tuning an already trained model. We selected FasterRCNN which is an end convolution neural Region-based Convolutional Neural Network. Before diving into the details of the model and our implementation, let's first explore the data.\n\nIn terms exploratory data analysis, every member was assigned the task to dig dipped in the data as much as he/she could in order to try pick up any patterns.\n","metadata":{}},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)\n\nUnderstanding the dataset, provided by [3].\n\nThe dataset consists of folder with the different images split in different folders corresponding to which video they belong. \n\nAlong side these images, we have train.csv file that contains the different annotations and meta data for the all the images.\n\nNo duplicated images nor corrupted data were found.  \n","metadata":{}},{"cell_type":"code","source":"# import some necessary packages\nimport ast\nimport numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sys\nimport os\n\nfrom os import listdir, path\nfrom PIL import Image, ImageDraw","metadata":{"execution":{"iopub.status.busy":"2022-02-09T11:45:44.165093Z","iopub.execute_input":"2022-02-09T11:45:44.165775Z","iopub.status.idle":"2022-02-09T11:45:45.056749Z","shell.execute_reply.started":"2022-02-09T11:45:44.165741Z","shell.execute_reply":"2022-02-09T11:45:45.056022Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Accessing the train dataset.\ndata_dir = '../input/tensorflow-great-barrier-reef/'\ndf_train = pd.read_csv(data_dir + 'train.csv')\ndf_train","metadata":{"execution":{"iopub.status.busy":"2022-02-09T11:45:45.059254Z","iopub.execute_input":"2022-02-09T11:45:45.059941Z","iopub.status.idle":"2022-02-09T11:45:45.129609Z","shell.execute_reply.started":"2022-02-09T11:45:45.059901Z","shell.execute_reply":"2022-02-09T11:45:45.128882Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"       video_id  sequence  video_frame  sequence_frame image_id annotations\n0             0     40258            0               0      0-0          []\n1             0     40258            1               1      0-1          []\n2             0     40258            2               2      0-2          []\n3             0     40258            3               3      0-3          []\n4             0     40258            4               4      0-4          []\n...         ...       ...          ...             ...      ...         ...\n23496         2     29859        10755            2983  2-10755          []\n23497         2     29859        10756            2984  2-10756          []\n23498         2     29859        10757            2985  2-10757          []\n23499         2     29859        10758            2986  2-10758          []\n23500         2     29859        10759            2987  2-10759          []\n\n[23501 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_id</th>\n      <th>sequence</th>\n      <th>video_frame</th>\n      <th>sequence_frame</th>\n      <th>image_id</th>\n      <th>annotations</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>40258</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0-0</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>40258</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0-1</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>40258</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0-2</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>40258</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0-3</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>40258</td>\n      <td>4</td>\n      <td>4</td>\n      <td>0-4</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23496</th>\n      <td>2</td>\n      <td>29859</td>\n      <td>10755</td>\n      <td>2983</td>\n      <td>2-10755</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>23497</th>\n      <td>2</td>\n      <td>29859</td>\n      <td>10756</td>\n      <td>2984</td>\n      <td>2-10756</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>23498</th>\n      <td>2</td>\n      <td>29859</td>\n      <td>10757</td>\n      <td>2985</td>\n      <td>2-10757</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>23499</th>\n      <td>2</td>\n      <td>29859</td>\n      <td>10758</td>\n      <td>2986</td>\n      <td>2-10758</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>23500</th>\n      <td>2</td>\n      <td>29859</td>\n      <td>10759</td>\n      <td>2987</td>\n      <td>2-10759</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n<p>23501 rows Ã— 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T11:45:45.130800Z","iopub.execute_input":"2022-02-09T11:45:45.131028Z","iopub.status.idle":"2022-02-09T11:45:45.158006Z","shell.execute_reply.started":"2022-02-09T11:45:45.130995Z","shell.execute_reply":"2022-02-09T11:45:45.157139Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 23501 entries, 0 to 23500\nData columns (total 6 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   video_id        23501 non-null  int64 \n 1   sequence        23501 non-null  int64 \n 2   video_frame     23501 non-null  int64 \n 3   sequence_frame  23501 non-null  int64 \n 4   image_id        23501 non-null  object\n 5   annotations     23501 non-null  object\ndtypes: int64(4), object(2)\nmemory usage: 1.1+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"# Checking if any of the columns have missing values\nprint(df_train.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2022-02-09T11:45:45.160055Z","iopub.execute_input":"2022-02-09T11:45:45.160368Z","iopub.status.idle":"2022-02-09T11:45:45.172937Z","shell.execute_reply.started":"2022-02-09T11:45:45.160331Z","shell.execute_reply":"2022-02-09T11:45:45.172081Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"video_id          0\nsequence          0\nvideo_frame       0\nsequence_frame    0\nimage_id          0\nannotations       0\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"# Checking the first 10 elements of the dataframe\ndf_train.head(10).style.background_gradient(cmap='Reds')","metadata":{"execution":{"iopub.status.busy":"2022-02-09T11:45:45.174403Z","iopub.execute_input":"2022-02-09T11:45:45.174818Z","iopub.status.idle":"2022-02-09T11:45:45.243664Z","shell.execute_reply.started":"2022-02-09T11:45:45.174761Z","shell.execute_reply":"2022-02-09T11:45:45.243015Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7fcf53f8a7d0>","text/html":"<style type=\"text/css\">\n#T_4a192_row0_col0, #T_4a192_row0_col1, #T_4a192_row0_col2, #T_4a192_row0_col3, #T_4a192_row1_col0, #T_4a192_row1_col1, #T_4a192_row2_col0, #T_4a192_row2_col1, #T_4a192_row3_col0, #T_4a192_row3_col1, #T_4a192_row4_col0, #T_4a192_row4_col1, #T_4a192_row5_col0, #T_4a192_row5_col1, #T_4a192_row6_col0, #T_4a192_row6_col1, #T_4a192_row7_col0, #T_4a192_row7_col1, #T_4a192_row8_col0, #T_4a192_row8_col1, #T_4a192_row9_col0, #T_4a192_row9_col1 {\n  background-color: #fff5f0;\n  color: #000000;\n}\n#T_4a192_row1_col2, #T_4a192_row1_col3 {\n  background-color: #fee3d6;\n  color: #000000;\n}\n#T_4a192_row2_col2, #T_4a192_row2_col3 {\n  background-color: #fcc4ad;\n  color: #000000;\n}\n#T_4a192_row3_col2, #T_4a192_row3_col3 {\n  background-color: #fca082;\n  color: #000000;\n}\n#T_4a192_row4_col2, #T_4a192_row4_col3 {\n  background-color: #fb7c5c;\n  color: #f1f1f1;\n}\n#T_4a192_row5_col2, #T_4a192_row5_col3 {\n  background-color: #f6553c;\n  color: #f1f1f1;\n}\n#T_4a192_row6_col2, #T_4a192_row6_col3 {\n  background-color: #e32f27;\n  color: #f1f1f1;\n}\n#T_4a192_row7_col2, #T_4a192_row7_col3 {\n  background-color: #c2161b;\n  color: #f1f1f1;\n}\n#T_4a192_row8_col2, #T_4a192_row8_col3 {\n  background-color: #9d0d14;\n  color: #f1f1f1;\n}\n#T_4a192_row9_col2, #T_4a192_row9_col3 {\n  background-color: #67000d;\n  color: #f1f1f1;\n}\n</style>\n<table id=\"T_4a192_\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th class=\"col_heading level0 col0\" >video_id</th>\n      <th class=\"col_heading level0 col1\" >sequence</th>\n      <th class=\"col_heading level0 col2\" >video_frame</th>\n      <th class=\"col_heading level0 col3\" >sequence_frame</th>\n      <th class=\"col_heading level0 col4\" >image_id</th>\n      <th class=\"col_heading level0 col5\" >annotations</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_4a192_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_4a192_row0_col0\" class=\"data row0 col0\" >0</td>\n      <td id=\"T_4a192_row0_col1\" class=\"data row0 col1\" >40258</td>\n      <td id=\"T_4a192_row0_col2\" class=\"data row0 col2\" >0</td>\n      <td id=\"T_4a192_row0_col3\" class=\"data row0 col3\" >0</td>\n      <td id=\"T_4a192_row0_col4\" class=\"data row0 col4\" >0-0</td>\n      <td id=\"T_4a192_row0_col5\" class=\"data row0 col5\" >[]</td>\n    </tr>\n    <tr>\n      <th id=\"T_4a192_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_4a192_row1_col0\" class=\"data row1 col0\" >0</td>\n      <td id=\"T_4a192_row1_col1\" class=\"data row1 col1\" >40258</td>\n      <td id=\"T_4a192_row1_col2\" class=\"data row1 col2\" >1</td>\n      <td id=\"T_4a192_row1_col3\" class=\"data row1 col3\" >1</td>\n      <td id=\"T_4a192_row1_col4\" class=\"data row1 col4\" >0-1</td>\n      <td id=\"T_4a192_row1_col5\" class=\"data row1 col5\" >[]</td>\n    </tr>\n    <tr>\n      <th id=\"T_4a192_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_4a192_row2_col0\" class=\"data row2 col0\" >0</td>\n      <td id=\"T_4a192_row2_col1\" class=\"data row2 col1\" >40258</td>\n      <td id=\"T_4a192_row2_col2\" class=\"data row2 col2\" >2</td>\n      <td id=\"T_4a192_row2_col3\" class=\"data row2 col3\" >2</td>\n      <td id=\"T_4a192_row2_col4\" class=\"data row2 col4\" >0-2</td>\n      <td id=\"T_4a192_row2_col5\" class=\"data row2 col5\" >[]</td>\n    </tr>\n    <tr>\n      <th id=\"T_4a192_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n      <td id=\"T_4a192_row3_col0\" class=\"data row3 col0\" >0</td>\n      <td id=\"T_4a192_row3_col1\" class=\"data row3 col1\" >40258</td>\n      <td id=\"T_4a192_row3_col2\" class=\"data row3 col2\" >3</td>\n      <td id=\"T_4a192_row3_col3\" class=\"data row3 col3\" >3</td>\n      <td id=\"T_4a192_row3_col4\" class=\"data row3 col4\" >0-3</td>\n      <td id=\"T_4a192_row3_col5\" class=\"data row3 col5\" >[]</td>\n    </tr>\n    <tr>\n      <th id=\"T_4a192_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n      <td id=\"T_4a192_row4_col0\" class=\"data row4 col0\" >0</td>\n      <td id=\"T_4a192_row4_col1\" class=\"data row4 col1\" >40258</td>\n      <td id=\"T_4a192_row4_col2\" class=\"data row4 col2\" >4</td>\n      <td id=\"T_4a192_row4_col3\" class=\"data row4 col3\" >4</td>\n      <td id=\"T_4a192_row4_col4\" class=\"data row4 col4\" >0-4</td>\n      <td id=\"T_4a192_row4_col5\" class=\"data row4 col5\" >[]</td>\n    </tr>\n    <tr>\n      <th id=\"T_4a192_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n      <td id=\"T_4a192_row5_col0\" class=\"data row5 col0\" >0</td>\n      <td id=\"T_4a192_row5_col1\" class=\"data row5 col1\" >40258</td>\n      <td id=\"T_4a192_row5_col2\" class=\"data row5 col2\" >5</td>\n      <td id=\"T_4a192_row5_col3\" class=\"data row5 col3\" >5</td>\n      <td id=\"T_4a192_row5_col4\" class=\"data row5 col4\" >0-5</td>\n      <td id=\"T_4a192_row5_col5\" class=\"data row5 col5\" >[]</td>\n    </tr>\n    <tr>\n      <th id=\"T_4a192_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n      <td id=\"T_4a192_row6_col0\" class=\"data row6 col0\" >0</td>\n      <td id=\"T_4a192_row6_col1\" class=\"data row6 col1\" >40258</td>\n      <td id=\"T_4a192_row6_col2\" class=\"data row6 col2\" >6</td>\n      <td id=\"T_4a192_row6_col3\" class=\"data row6 col3\" >6</td>\n      <td id=\"T_4a192_row6_col4\" class=\"data row6 col4\" >0-6</td>\n      <td id=\"T_4a192_row6_col5\" class=\"data row6 col5\" >[]</td>\n    </tr>\n    <tr>\n      <th id=\"T_4a192_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n      <td id=\"T_4a192_row7_col0\" class=\"data row7 col0\" >0</td>\n      <td id=\"T_4a192_row7_col1\" class=\"data row7 col1\" >40258</td>\n      <td id=\"T_4a192_row7_col2\" class=\"data row7 col2\" >7</td>\n      <td id=\"T_4a192_row7_col3\" class=\"data row7 col3\" >7</td>\n      <td id=\"T_4a192_row7_col4\" class=\"data row7 col4\" >0-7</td>\n      <td id=\"T_4a192_row7_col5\" class=\"data row7 col5\" >[]</td>\n    </tr>\n    <tr>\n      <th id=\"T_4a192_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n      <td id=\"T_4a192_row8_col0\" class=\"data row8 col0\" >0</td>\n      <td id=\"T_4a192_row8_col1\" class=\"data row8 col1\" >40258</td>\n      <td id=\"T_4a192_row8_col2\" class=\"data row8 col2\" >8</td>\n      <td id=\"T_4a192_row8_col3\" class=\"data row8 col3\" >8</td>\n      <td id=\"T_4a192_row8_col4\" class=\"data row8 col4\" >0-8</td>\n      <td id=\"T_4a192_row8_col5\" class=\"data row8 col5\" >[]</td>\n    </tr>\n    <tr>\n      <th id=\"T_4a192_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n      <td id=\"T_4a192_row9_col0\" class=\"data row9 col0\" >0</td>\n      <td id=\"T_4a192_row9_col1\" class=\"data row9 col1\" >40258</td>\n      <td id=\"T_4a192_row9_col2\" class=\"data row9 col2\" >9</td>\n      <td id=\"T_4a192_row9_col3\" class=\"data row9 col3\" >9</td>\n      <td id=\"T_4a192_row9_col4\" class=\"data row9 col4\" >0-9</td>\n      <td id=\"T_4a192_row9_col5\" class=\"data row9 col5\" >[]</td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"# Checking the last 10 elements of the dataframe\ndf_train.tail(10).style.background_gradient(cmap='Reds')","metadata":{"execution":{"iopub.status.busy":"2022-02-09T11:45:45.244998Z","iopub.execute_input":"2022-02-09T11:45:45.245457Z","iopub.status.idle":"2022-02-09T11:45:45.264213Z","shell.execute_reply.started":"2022-02-09T11:45:45.245422Z","shell.execute_reply":"2022-02-09T11:45:45.263510Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7fcf53f63f50>","text/html":"<style type=\"text/css\">\n#T_10955_row0_col0, #T_10955_row0_col1, #T_10955_row0_col2, #T_10955_row0_col3, #T_10955_row1_col0, #T_10955_row1_col1, #T_10955_row2_col0, #T_10955_row2_col1, #T_10955_row3_col0, #T_10955_row3_col1, #T_10955_row4_col0, #T_10955_row4_col1, #T_10955_row5_col0, #T_10955_row5_col1, #T_10955_row6_col0, #T_10955_row6_col1, #T_10955_row7_col0, #T_10955_row7_col1, #T_10955_row8_col0, #T_10955_row8_col1, #T_10955_row9_col0, #T_10955_row9_col1 {\n  background-color: #fff5f0;\n  color: #000000;\n}\n#T_10955_row1_col2, #T_10955_row1_col3 {\n  background-color: #fee3d6;\n  color: #000000;\n}\n#T_10955_row2_col2, #T_10955_row2_col3 {\n  background-color: #fcc4ad;\n  color: #000000;\n}\n#T_10955_row3_col2, #T_10955_row3_col3 {\n  background-color: #fca082;\n  color: #000000;\n}\n#T_10955_row4_col2, #T_10955_row4_col3 {\n  background-color: #fb7c5c;\n  color: #f1f1f1;\n}\n#T_10955_row5_col2, #T_10955_row5_col3 {\n  background-color: #f6553c;\n  color: #f1f1f1;\n}\n#T_10955_row6_col2, #T_10955_row6_col3 {\n  background-color: #e32f27;\n  color: #f1f1f1;\n}\n#T_10955_row7_col2, #T_10955_row7_col3 {\n  background-color: #c2161b;\n  color: #f1f1f1;\n}\n#T_10955_row8_col2, #T_10955_row8_col3 {\n  background-color: #9d0d14;\n  color: #f1f1f1;\n}\n#T_10955_row9_col2, #T_10955_row9_col3 {\n  background-color: #67000d;\n  color: #f1f1f1;\n}\n</style>\n<table id=\"T_10955_\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th class=\"col_heading level0 col0\" >video_id</th>\n      <th class=\"col_heading level0 col1\" >sequence</th>\n      <th class=\"col_heading level0 col2\" >video_frame</th>\n      <th class=\"col_heading level0 col3\" >sequence_frame</th>\n      <th class=\"col_heading level0 col4\" >image_id</th>\n      <th class=\"col_heading level0 col5\" >annotations</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_10955_level0_row0\" class=\"row_heading level0 row0\" >23491</th>\n      <td id=\"T_10955_row0_col0\" class=\"data row0 col0\" >2</td>\n      <td id=\"T_10955_row0_col1\" class=\"data row0 col1\" >29859</td>\n      <td id=\"T_10955_row0_col2\" class=\"data row0 col2\" >10750</td>\n      <td id=\"T_10955_row0_col3\" class=\"data row0 col3\" >2978</td>\n      <td id=\"T_10955_row0_col4\" class=\"data row0 col4\" >2-10750</td>\n      <td id=\"T_10955_row0_col5\" class=\"data row0 col5\" >[]</td>\n    </tr>\n    <tr>\n      <th id=\"T_10955_level0_row1\" class=\"row_heading level0 row1\" >23492</th>\n      <td id=\"T_10955_row1_col0\" class=\"data row1 col0\" >2</td>\n      <td id=\"T_10955_row1_col1\" class=\"data row1 col1\" >29859</td>\n      <td id=\"T_10955_row1_col2\" class=\"data row1 col2\" >10751</td>\n      <td id=\"T_10955_row1_col3\" class=\"data row1 col3\" >2979</td>\n      <td id=\"T_10955_row1_col4\" class=\"data row1 col4\" >2-10751</td>\n      <td id=\"T_10955_row1_col5\" class=\"data row1 col5\" >[]</td>\n    </tr>\n    <tr>\n      <th id=\"T_10955_level0_row2\" class=\"row_heading level0 row2\" >23493</th>\n      <td id=\"T_10955_row2_col0\" class=\"data row2 col0\" >2</td>\n      <td id=\"T_10955_row2_col1\" class=\"data row2 col1\" >29859</td>\n      <td id=\"T_10955_row2_col2\" class=\"data row2 col2\" >10752</td>\n      <td id=\"T_10955_row2_col3\" class=\"data row2 col3\" >2980</td>\n      <td id=\"T_10955_row2_col4\" class=\"data row2 col4\" >2-10752</td>\n      <td id=\"T_10955_row2_col5\" class=\"data row2 col5\" >[]</td>\n    </tr>\n    <tr>\n      <th id=\"T_10955_level0_row3\" class=\"row_heading level0 row3\" >23494</th>\n      <td id=\"T_10955_row3_col0\" class=\"data row3 col0\" >2</td>\n      <td id=\"T_10955_row3_col1\" class=\"data row3 col1\" >29859</td>\n      <td id=\"T_10955_row3_col2\" class=\"data row3 col2\" >10753</td>\n      <td id=\"T_10955_row3_col3\" class=\"data row3 col3\" >2981</td>\n      <td id=\"T_10955_row3_col4\" class=\"data row3 col4\" >2-10753</td>\n      <td id=\"T_10955_row3_col5\" class=\"data row3 col5\" >[]</td>\n    </tr>\n    <tr>\n      <th id=\"T_10955_level0_row4\" class=\"row_heading level0 row4\" >23495</th>\n      <td id=\"T_10955_row4_col0\" class=\"data row4 col0\" >2</td>\n      <td id=\"T_10955_row4_col1\" class=\"data row4 col1\" >29859</td>\n      <td id=\"T_10955_row4_col2\" class=\"data row4 col2\" >10754</td>\n      <td id=\"T_10955_row4_col3\" class=\"data row4 col3\" >2982</td>\n      <td id=\"T_10955_row4_col4\" class=\"data row4 col4\" >2-10754</td>\n      <td id=\"T_10955_row4_col5\" class=\"data row4 col5\" >[]</td>\n    </tr>\n    <tr>\n      <th id=\"T_10955_level0_row5\" class=\"row_heading level0 row5\" >23496</th>\n      <td id=\"T_10955_row5_col0\" class=\"data row5 col0\" >2</td>\n      <td id=\"T_10955_row5_col1\" class=\"data row5 col1\" >29859</td>\n      <td id=\"T_10955_row5_col2\" class=\"data row5 col2\" >10755</td>\n      <td id=\"T_10955_row5_col3\" class=\"data row5 col3\" >2983</td>\n      <td id=\"T_10955_row5_col4\" class=\"data row5 col4\" >2-10755</td>\n      <td id=\"T_10955_row5_col5\" class=\"data row5 col5\" >[]</td>\n    </tr>\n    <tr>\n      <th id=\"T_10955_level0_row6\" class=\"row_heading level0 row6\" >23497</th>\n      <td id=\"T_10955_row6_col0\" class=\"data row6 col0\" >2</td>\n      <td id=\"T_10955_row6_col1\" class=\"data row6 col1\" >29859</td>\n      <td id=\"T_10955_row6_col2\" class=\"data row6 col2\" >10756</td>\n      <td id=\"T_10955_row6_col3\" class=\"data row6 col3\" >2984</td>\n      <td id=\"T_10955_row6_col4\" class=\"data row6 col4\" >2-10756</td>\n      <td id=\"T_10955_row6_col5\" class=\"data row6 col5\" >[]</td>\n    </tr>\n    <tr>\n      <th id=\"T_10955_level0_row7\" class=\"row_heading level0 row7\" >23498</th>\n      <td id=\"T_10955_row7_col0\" class=\"data row7 col0\" >2</td>\n      <td id=\"T_10955_row7_col1\" class=\"data row7 col1\" >29859</td>\n      <td id=\"T_10955_row7_col2\" class=\"data row7 col2\" >10757</td>\n      <td id=\"T_10955_row7_col3\" class=\"data row7 col3\" >2985</td>\n      <td id=\"T_10955_row7_col4\" class=\"data row7 col4\" >2-10757</td>\n      <td id=\"T_10955_row7_col5\" class=\"data row7 col5\" >[]</td>\n    </tr>\n    <tr>\n      <th id=\"T_10955_level0_row8\" class=\"row_heading level0 row8\" >23499</th>\n      <td id=\"T_10955_row8_col0\" class=\"data row8 col0\" >2</td>\n      <td id=\"T_10955_row8_col1\" class=\"data row8 col1\" >29859</td>\n      <td id=\"T_10955_row8_col2\" class=\"data row8 col2\" >10758</td>\n      <td id=\"T_10955_row8_col3\" class=\"data row8 col3\" >2986</td>\n      <td id=\"T_10955_row8_col4\" class=\"data row8 col4\" >2-10758</td>\n      <td id=\"T_10955_row8_col5\" class=\"data row8 col5\" >[]</td>\n    </tr>\n    <tr>\n      <th id=\"T_10955_level0_row9\" class=\"row_heading level0 row9\" >23500</th>\n      <td id=\"T_10955_row9_col0\" class=\"data row9 col0\" >2</td>\n      <td id=\"T_10955_row9_col1\" class=\"data row9 col1\" >29859</td>\n      <td id=\"T_10955_row9_col2\" class=\"data row9 col2\" >10759</td>\n      <td id=\"T_10955_row9_col3\" class=\"data row9 col3\" >2987</td>\n      <td id=\"T_10955_row9_col4\" class=\"data row9 col4\" >2-10759</td>\n      <td id=\"T_10955_row9_col5\" class=\"data row9 col5\" >[]</td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"# Accessing the test dataset \ndf_test = pd.read_csv(data_dir + 'test.csv')\ndf_test","metadata":{"execution":{"iopub.status.busy":"2022-02-09T11:45:45.265364Z","iopub.execute_input":"2022-02-09T11:45:45.265624Z","iopub.status.idle":"2022-02-09T11:45:45.286234Z","shell.execute_reply.started":"2022-02-09T11:45:45.265594Z","shell.execute_reply":"2022-02-09T11:45:45.285572Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   video_id  sequence  video_frame  sequence_frame image_id\n0         3     17063            0               0      3-0\n1         3     17063            1               1      3-1\n2         3     17063            2               2      3-2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_id</th>\n      <th>sequence</th>\n      <th>video_frame</th>\n      <th>sequence_frame</th>\n      <th>image_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>17063</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3-0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>17063</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3-1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>17063</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3-2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Understanding dataset through its statiscal distribution\ndf_train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T11:45:45.287460Z","iopub.execute_input":"2022-02-09T11:45:45.287862Z","iopub.status.idle":"2022-02-09T11:45:45.310290Z","shell.execute_reply.started":"2022-02-09T11:45:45.287828Z","shell.execute_reply":"2022-02-09T11:45:45.309569Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"           video_id      sequence   video_frame  sequence_frame\ncount  23501.000000  23501.000000  23501.000000    23501.000000\nmean       1.078848  33181.644994   5727.471257      974.231224\nstd        0.802202  18804.050915   3528.411913      795.594611\nmin        0.000000    996.000000      0.000000        0.000000\n25%        0.000000  15827.000000   2349.000000      336.000000\n50%        1.000000  29859.000000   5553.000000      740.000000\n75%        2.000000  45518.000000   9048.000000     1447.000000\nmax        2.000000  60754.000000  12347.000000     2987.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_id</th>\n      <th>sequence</th>\n      <th>video_frame</th>\n      <th>sequence_frame</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>23501.000000</td>\n      <td>23501.000000</td>\n      <td>23501.000000</td>\n      <td>23501.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.078848</td>\n      <td>33181.644994</td>\n      <td>5727.471257</td>\n      <td>974.231224</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.802202</td>\n      <td>18804.050915</td>\n      <td>3528.411913</td>\n      <td>795.594611</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>996.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>15827.000000</td>\n      <td>2349.000000</td>\n      <td>336.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.000000</td>\n      <td>29859.000000</td>\n      <td>5553.000000</td>\n      <td>740.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2.000000</td>\n      <td>45518.000000</td>\n      <td>9048.000000</td>\n      <td>1447.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.000000</td>\n      <td>60754.000000</td>\n      <td>12347.000000</td>\n      <td>2987.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Checking for duplicates\nprint('Number of duplicates present: ', df_train.duplicated().sum())\n#df_train.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T11:45:45.311716Z","iopub.execute_input":"2022-02-09T11:45:45.312307Z","iopub.status.idle":"2022-02-09T11:45:45.327934Z","shell.execute_reply.started":"2022-02-09T11:45:45.312262Z","shell.execute_reply":"2022-02-09T11:45:45.327262Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Number of duplicates present:  0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Checking for corrupted data\n# https://stackoverflow.com/questions/46854496/python-script-to-detect-broken-images\n\n# not sure if this is good practice to just loop images this way. We can handle this is a better way while training\n\ndef verify_images(video_id):\n    img_path = data_dir + 'train_images/video_{}/'.format(video_id)\n    \n    print('Verifying that video {} frames are valid...'.format(video_id))\n    for filename in listdir(img_path):\n        if filename.upper().endswith('.JPG'):  \n            try:\n                img = Image.open(os.path.join(img_path, filename))\n                img.verify() # Verify it is in fact an image\n            except (IOError, SyntaxError) as e:\n                print('Bad file: ', filename) \n                #os.remove(img)\n    print('Verified! Video {} has no corrupted images'.format(video_id))\n\nfor video_id in range(3):\n    verify_images(video_id)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T11:45:45.330937Z","iopub.execute_input":"2022-02-09T11:45:45.331128Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Verifying that video 0 frames are valid...\nVerified! Video 0 has no corrupted images\nVerifying that video 1 frames are valid...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Checking number of images with bounding boxes (i.e. with at least one start fish)","metadata":{}},{"cell_type":"code","source":"df_train_boxes = df_train[df_train.annotations != '[]']\nlen(df_train_boxes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding image with maximum number of bounding boxes\nmax_boxes = df_train['annotations'].str.count('x').max()\nimage_with_max_boxes = df_train['annotations'].str.count('x').idxmax()\nprint('Maximum number of boxes: ', max_boxes)\nprint('Corresponding image number: ', image_with_max_boxes)\nprint(df_train.iloc[image_with_max_boxes])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining function to show values on top of bars for barplots\n# https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\ndef show_values_on_bars(axs, h_v=\"v\", space=0.4):\n    def _show_on_single_plot(ax):\n        if h_v == \"v\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() / 2\n                _y = p.get_y() + p.get_height()\n                value = int(p.get_height())\n                ax.text(_x, _y, value, ha=\"center\") \n        elif h_v == \"h\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() + float(space)\n                _y = p.get_y() + p.get_height()\n                value = int(p.get_width())\n                ax.text(_x, _y, value, ha=\"left\")\n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Plotting total and annotated-only dataframes     \nf, axs = plt.subplots(1,2,\n                      figsize=(16,8),\n                      sharey=True)\n\ndf1 = df_train[\"video_id\"].value_counts().reset_index()\ndf2 = df_train_boxes[\"video_id\"].value_counts().reset_index()\n\nsns.barplot(data=df1, x=\"index\", y=\"video_id\", ax=axs[0],\n            palette='Set2')\nshow_values_on_bars(axs[0], h_v=\"v\", space=0.1)\naxs[0].set_xlabel(\"\")\naxs[0].set_ylabel(\"\")\n\nsns.barplot(data=df2, x=\"index\", y=\"video_id\", ax=axs[1],\n            palette='Set2')\nshow_values_on_bars(axs[1], h_v=\"v\", space=0.1)\naxs[1].set_xlabel(\"\")\naxs[1].set_ylabel(\"\")\n\nf.text(0.25, 1, 'Total number of images per video directory', ha='center')\nf.text(0.75, 1, 'Number of annotated images per video directory', ha='center')\nf.text(0.25, -0.05, 'Video ID', ha='center')\nf.text(0.75, -0.05, 'Video ID', ha='center')\nf.text(0, 0.5, 'Number of images', va='center', rotation='vertical')\nf.text(0, 0, '')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Statistics of the above\ndef percentage_of(part, whole):\n    percentage_res = (part/whole)*100\n    return np.round(percentage_res, 1)\n\ntotal_img = len(df_train)\nno_img_0 = len(df_train[df_train[\"video_id\"] == 0])\nno_img_1 = len(df_train[df_train[\"video_id\"] == 1])\nno_img_2 = len(df_train[df_train[\"video_id\"] == 2])\nprint('Video ID 0 makes up {}% of the total amount of videos.'.format(percentage_of(no_img_0, total_img)))\nprint('Video ID 1 makes up {}% of the total amount of videos.'.format(percentage_of(no_img_1, total_img)))\nprint('Video ID 2 makes up {}% of the total amount of videos.'.format(percentage_of(no_img_2, total_img)))\n\nprint()\ntotal_img_anno = len(df_train_boxes)\nno_img_0_anno = len(df_train_boxes[df_train_boxes[\"video_id\"] == 0])\nno_img_1_anno = len(df_train_boxes[df_train_boxes[\"video_id\"] == 1])\nno_img_2_anno = len(df_train_boxes[df_train_boxes[\"video_id\"] == 2])\nprint('Video ID 0 has {}% of its videos annotated.'.format(percentage_of(no_img_0_anno, no_img_0)))\nprint('Video ID 1 has {}% of its videos annotated.'.format(percentage_of(no_img_1_anno, no_img_1)))\nprint('Video ID 2 has {}% of its videos annotated.'.format(percentage_of(no_img_2_anno, no_img_2)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the number of total annotations within the frame\ndf_train[\"no_annotations\"] = df_train[\"annotations\"].apply(lambda x: len(eval(x)))\n#df_train[\"no_annotations\"].head(25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# % annotations\nn = len(df_train)\nno_annot = round(df_train[df_train.no_annotations==0].shape[0]/n*100)\nwith_annot = round(df_train[df_train.no_annotations!=0].shape[0]/n*100)\n\nprint(f\"There are about {no_annot}% frames with no annotations and\",\n      \"\\n\",\n      f\"about {with_annot}% frames with at least 1 annotation.\")\nprint()\n\n# Plot\nplt.figure(figsize=(23, 10))\nsns.histplot(df_train[\"no_annotations\"], bins=(np.arange(18)-0.5), kde=True, element=\"step\", \n             color='b')\n\nplt.xlabel(\"Number of annotations\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Distribution of annotations per image frame\")\nplt.xticks(range(0,20,1))\n\nsns.despine(top=True, right=True, left=False, bottom=True)\n\n\nprint('It can be seen from the histogram that most annotated images have 1 or 2 bounding boxes.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting image with highest amount of bounding boxes\n\ndef fetch_image_list(df_tmp, video_id, num_images, start_frame_idx):\n    def fetch_image(frame_id):\n        path_base = data_dir + 'train_images/video_{}/{}.jpg'\n        raw_img = Image.open(path_base.format(video_id, frame_id))\n\n        row_frame = df_tmp[(df_tmp.video_id == video_id) & (df_tmp.video_frame == frame_id)].iloc[0]\n        bounding_boxes = ast.literal_eval(row_frame.annotations)\n\n        for box in bounding_boxes:\n            draw = ImageDraw.Draw(raw_img)\n            x0, y0, x1, y1 = (box['x'], box['y'], box['x']+box['width'], box['y']+box['height'])\n            draw.rectangle( (x0, y0, x1, y1), outline=180, width=3)\n        return raw_img\n\n    return [np.array(fetch_image(start_frame_idx + index)) for index in range(num_images)]\n\nimages = fetch_image_list(df_train, video_id = 1, num_images = 1, start_frame_idx = 9114)\n\nprint('Image Number: ', 9114)\nplt.figure(figsize=(33, 12))\nplt.imshow(images[0], interpolation='nearest')\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the baseline model\n\nWe will not run the cells in this training section. We will just explain what these are supposed to do.","metadata":{}},{"cell_type":"markdown","source":"Before we settled on using PyTorch, we trained the model using Tensorflow and Keras libraries but PyTorch performed better. We used the FasterR-CNN pre-trained model and fine-tuned it as per the requirements of this project. One of the first steps when preparing to train a model is build a dataloader than can feed in the model with data at every step of the training. Hence, our first step was to build a PyTorch Dataset loader.\n\nPyTorch uses torch.utils.data.DataLoader and torch.utils.data.Dataset class to work with data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset. For this object detection project, we will be using a TorchVision dataset. To declare, initialize, and manipulate objects in Python, we use classes. The getitem reads the image using the image_id we have in the dataframe, and also we can get all the bounding boxes associated with that image. We then initialize a dict called target, which will be passed to model for training. This target will have metadata of the annotation like actual bounding box coordinates, its corresponding labels, image_id, area of the bounding boxes. The area parameter is used during evaluation with the COCO metric, to separate the metric scores between small, medium, and large boxes. If we set iscrowd as True, those instances will be ignored during evaluation. The len method gives the size of the Dataset.","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n<b>NOTE:</b> Since we trained our on model on Kaggle, unfortunately we had to re-install certain packages not available on Kaggle. The pycocotools package used by our model, its release version on PyPi is broken, so we had to install a working branch from github. The next cells are pip installs of the necessary libraries, and also adding of utility scripts to our programming environment. As we said earlier, we will not run any of these training cells in our report notebook.\n</div>","metadata":{}},{"cell_type":"code","source":"# Try doing all the installations here\n!pip install -I numpy\n\n!pip install -I torchvision\n!pip install -I torch -U   \n\n# First, we need to install pycocotools. This library will be used for computing the evaluation metrics following the COCO metric for intersection over union.\n\n!pip install cython\n# Install pycocotools, the version by default in Colab\n# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n!pip install -I 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI' --no-binary pycocotools\n!ls /kaggle/input/baseline-predict-pytorch\n\n!git clone https://github.com/pytorch/vision.git\n!cd vision\n!git checkout v0.8.2\n\n\n!cp vision/references/detection/utils.py /kaggle/working/\n!cp vision/references/detection/transforms.py /kaggle/working/\n!cp vision/references/detection/coco_eval.py /kaggle/working/\n!cp vision/references/detection/engine.py /kaggle/working/\n!cp vision/references/detection/coco_utils.py /kaggle/working/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport ast\nimport sys\nimport os\n\n# os.environ['TORCH_HOME'] = '\\\\kaggle\\\\input\\\\resnet'\n\nimport glob\nimport sklearn\nimport math\nimport random\n\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom transformers import get_cosine_schedule_with_warmup\n\nfrom PIL import Image\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn import metrics, model_selection, preprocessing\nfrom sklearn.model_selection import GroupKFold\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# These are some utilties functions for plotting the image.\nimport cv2\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches\n\ndef get_rectangle_edges_from_pascal_bbox(bbox):\n    xmin_top_left, ymin_top_left, xmax_bottom_right, ymax_bottom_right = bbox\n\n    bottom_left = (xmin_top_left, ymax_bottom_right)\n    width = xmax_bottom_right - xmin_top_left\n    height = ymin_top_left - ymax_bottom_right\n\n    return bottom_left, width, height\n\ndef draw_pascal_voc_bboxes(\n    plot_ax,\n    bboxes,\n    get_rectangle_corners_fn=get_rectangle_edges_from_pascal_bbox,\n    color='red'\n):\n    for bbox in bboxes:\n        bottom_left, width, height = get_rectangle_corners_fn(bbox)\n\n        rect_1 = patches.Rectangle(\n            bottom_left,\n            width,\n            height,\n            linewidth=2,\n            edgecolor=\"black\",\n            fill=False,\n        )\n        rect_2 = patches.Rectangle(\n            bottom_left,\n            width,\n            height,\n            linewidth=2,\n            edgecolor=color,\n            fill=False,\n        )\n\n        # Add the patch to the Axes\n        plot_ax.add_patch(rect_1)\n        plot_ax.add_patch(rect_2)\n\ndef draw_image(\n    image, bboxes=None, pboxes=None, image_id=None, color2='blue', draw_bboxes_fn=draw_pascal_voc_bboxes, figsize=(10, 10)\n):\n    fig, ax = plt.subplots(1, figsize=figsize)\n    ax.imshow(image)\n    if image_id is not None:\n        ax.set_title(f\"(Tp, Fn, Fp) = {image_id}\")\n\n    if bboxes is not None:\n        draw_bboxes_fn(ax, bboxes, color='red')\n    if pboxes is not None:\n        draw_bboxes_fn(ax, pboxes, color=color2)\n\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataAdaptor:\n    def __init__(self,df):\n        self.df = df\n    def __len__(self):\n        return len(self.df)\n    \n    def get_boxes(self, row):\n        \"\"\"Returns the bboxes for a given row as a 3D matrix with format [x_min, y_min, x_max, y_max]\"\"\"\n        \n        boxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\n        \n        # Change from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n        boxes[:, 2] = np.clip(boxes[:, 0] + boxes[:, 2],0,1280)\n        boxes[:, 3] = np.clip(boxes[:, 1] + boxes[:, 3],0,720) \n        \n        return boxes\n    \n    def get_image_bb(self , idx):\n        img_src = self.df.loc[idx,'path']\n        image   = cv2.imread(img_src)\n        image   = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        row     = self.df.iloc[idx]\n        bboxes  = self.get_boxes(row) \n        class_labels = np.ones(len(bboxes))\n        return image, bboxes, class_labels, idx\n    \n    def show_overlay(self, image, boxes, pboxes, image_id, color2=\"blue\"):\n        draw_image(image, bboxes.tolist(), pboxes.tolist(), image_id=image_id, color2=color2)\n        \n    def show_image(self, index):\n        image, bboxes, class_labels, image_id = self.get_image_bb(index)\n        print(f\"image_id: {image_id}\")\n        draw_image(image, bboxes.tolist())\n#         print(class_labels) \n        return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.utils.data\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch import optim\nfrom torchvision import transforms\n\nfrom typing import List, Dict\n\n\n# This is our main pytorch dataset class.\n# This is initialise from a pandas data frame\nclass CotsData(torch.utils.data.Dataset):\n    def __init__(self, df, transforms=None):\n        self.ds = df\n        self.transforms = transforms\n    \n    def get_boxes(self, row):\n        \"\"\"Returns the bboxes for a given row as a 3D matrix with format [x_min, y_min, x_max, y_max]\"\"\"\n        \n        boxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\n        \n        # Change from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n        boxes[:, 2] = np.clip(boxes[:, 0] + boxes[:, 2],0,1280)\n        boxes[:, 3] = np.clip(boxes[:, 1] + boxes[:, 3],0,720) \n        \n        return boxes\n            \n    def __getitem__(self, idx):\n        # load images\n        img_path = self.ds.loc[idx,'path']\n        # mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        # note that we haven't converted the mask to RGB,\n        \n        row = self.ds.iloc[idx]\n        boxes = self.get_boxes(row)\n        num_objs = self.ds.loc[idx, 'number_boxes']\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        \n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64) # check this probably have to set this to true\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train only on images with detections let see\ndf = pd.read_csv(\"../input/tensorflow-great-barrier-reef/train.csv\")\ndf['path'] = [f\"../input/tensorflow-great-barrier-reef/train_images/video_{a}/{b}.jpg\" for a,b in zip(df[\"video_id\"],df[\"video_frame\"])]\ndf['annotations'] = df['annotations'].apply(eval)\ndf['number_boxes'] = df['annotations'].apply(lambda x: len(x))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training, Validation Datasets and Data Loaders.\n\nThe next step is to create our training and validation datasets. The data loader loads the training data in batches into the model for training. For this also we will be using PyTorchâ€™s DataLoader utility. From the EDA we observed that most of the images did not have annotations. Also, we realised after previous training that the sequence was important. So we decided to use the images from video 0 and video 1 for training and use Video 2 for testing. Also we tried to balance the dataset by dropping most of the images without annotations. Overall, we used all the images with annotations. For training, we added 1000 images without annotations, and 100 images without annotations for testing.","metadata":{}},{"cell_type":"code","source":"# get training and validation dataframes\ndef get_train_val(df, train=True):\n    if train:\n        df2 = df[df.video_id != 2]\n        dfn = df2[df.number_boxes>0]\n        dfo = df2[df.number_boxes==0]\n        dfno = dfo.sample(n=1000, replace=False, random_state=1)\n        result = pd.concat([dfn, dfno])\n    else:\n        df2 = df[df.video_id==2]\n        dfn = df2[df.number_boxes>0]\n        dfo = df2[df.number_boxes==0]\n        dfno = dfo.sample(n=100, replace=False, random_state=1)\n        result = pd.concat([dfn, dfno])\n    \n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fold_n = 1\n# train_df= df[df.fold != fold_n]\n# val_df  = df[df.fold == fold_n]\n\ntrain_df = get_train_val(df, train=True)\nval_df = get_train_val(df, train=False)\n\n# use our dataset and defined transformations\ndataset = CotsData(train_df.reset_index(drop=True), get_transform(train=True))\ndataset_test = CotsData(val_df.reset_index(drop=True), get_transform(train=False))\n# split the dataset in train and test set\ntorch.manual_seed(1)\n# indices = torch.randperm(len(dataset)).tolist()\n# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=8, shuffle=True, num_workers=4,\n    collate_fn=utils.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n    collate_fn=utils.collate_fn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from engine import train_one_epoch, evaluate\nimport utils\nimport transforms as T\n\n\ndef get_transform(train):\n    transforms = []\n    # converts the image, a PIL image, into a PyTorch Tensor\n    transforms.append(T.ToTensor())\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        transforms.append(T.RandomHorizontalFlip(0.5))\n#         transforms.append(T.RandomPhotometricDistort())\n#         transforms.append(T.RandomZoomOut())\n    return T.Compose(transforms)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Faster R-CNN\nThe Faster R-CNN profoundly replaces the Selective Search technique with much efficient Region Proposal Network that generates the detected areas in an image. It is highly used for carrying out real-time performance-based tasks in object detection tasks. We initialize our model using torchvisionâ€™s FasterRCNN and investigate different backbones namely:\n- Resnet50\n- Mobilenet v3\n- Mobilenet v3 320\n   \nThey were trained with all other variables being equalled, and the two mobilenet trainers produced the least good Kaggle scores, with Mobilenet v3 320 being the worst. Thus, the choice was made to continue with Resnet50. Furtheremore, some brief hyperparameter fine-tuning was carried out. It was found that an epoch number of 20 produced the best result when compared to epoch numbers of 1, 10, 30, and 40. Longer epochs of 30 and 40 produced less good results mostly likely due to overfitting. Another variable which was investigated was the backbone number, where a default number of 3 produced better results than that of 4. Again, this is most likely due to overfitting occuring at a higher backbone number producing a worse result.  \n\nThe optimizer we are using here is SGD (Stochastic Gradient Descent). The learning rate scheduler helps to adjust the learning rate during the course of the training to achieve more accuracy and speed up convergence. We use StepLR scheduler which decays the learning rate of each parameter group by gamma every step_size epochs. The gamma and step_size hyperparameters will decide the learning rate decay. \n\n\n## Augmentations\nWe tried different Augmentations namely, Horizontal flip, RandomPhotometricDistort, RandomZoomOut. Horizontal flip gave the best results and continued with it. But it should be noted we could really explore more of these.\n\n\n## Hyperparameter fine-tuning\nOther than the number of epochs and number of layers in the backbone pretrained Resnet50 model, we did not have enough time to perform an extensive hyperparameter fine-tuning. Also were trying to explore different models and parameters of our model that could improve the model, before fine-tuning the hyperparameters of our best model.","metadata":{}},{"cell_type":"code","source":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n      \ndef get_instance_segmentation_model(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) # trainable_backbone_layers=4\n    #model.load_state_dict(torch.load('/kaggle/input/resnet/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth'))\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    #hidden_layer = 256\n\n    return model\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n\n# get the model using our helper function\nmodel = get_instance_segmentation_model(num_classes)\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.05,\n                            momentum=0.9, weight_decay=0.0005)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.optim.lr_scheduler import StepLR\nnum_epochs = 20\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, data_loader_test, device=device)\n\ntorch.save(model.state_dict(), 'checkpoint-video2.pth')\n\n\nimport os\nos.chdir(r'../working')\nfrom IPython.display import FileLink\nFileLink(r'checkpoint-video2.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis of the model","metadata":{}},{"cell_type":"markdown","source":"After training the model is saved as a PyTorch dictionary class of weights. Hence, we can re-initialise the model using the saved dictionry for prediction and analysis.","metadata":{}},{"cell_type":"code","source":"# Again we have to add the same utility scripts we used for training here\n!cp /kaggle/input/k/ulricharmel/baseline-predict-pytorch/utils.py /kaggle/working/\n!cp /kaggle/input/k/ulricharmel/baseline-predict-pytorch/transforms.py /kaggle/working/\n!cp /kaggle/input/k/ulricharmel/baseline-predict-pytorch/coco_eval.py /kaggle/working/\n!cp /kaggle/input/k/ulricharmel/baseline-predict-pytorch/engine.py /kaggle/working/\n!cp /kaggle/input/k/ulricharmel/baseline-predict-pytorch/coco_utils.py /kaggle/working/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n      \ndef get_instance_segmentation_model(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False) #pretrained=True\n    model.load_state_dict(torch.load('/kaggle/input/resnet/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth'))\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from engine import train_one_epoch, evaluate\n# import utils\nimport transforms as T\n\n\ndef get_transform(train):\n    transforms = []\n    # converts the image, a PIL image, into a PyTorch Tensor\n    transforms.append(T.ToTensor())\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we redefine the model\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n\n# get the model using our helper function\nmodel = get_instance_segmentation_model(num_classes)\n# move model to the right device\nmodel.to(device)\n\n# This loads the model\nmodel_path = '/kaggle/input/checkvideo2/checkpoint-video2.pth'\nstate_dict = torch.load(model_path)\n# print(state_dict.keys())\nmodel.load_state_dict(state_dict)\nmodel.eval()\n\nprint(\"Nothing important, just to prevent printing the whole model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_nms(orig_prediction, iou_thresh=0.3, score_thresh=0.3):\n    \n    # torchvision returns the indices of the bboxes to keep\n    # function to implement non maximum suppression\n    # might also need to eliminate predictions with very low scores\n    # trim low scores first\n    \n    keep = orig_prediction['scores'] >= score_thresh\n    \n    scores_prediction = {}\n    scores_prediction['boxes'] = orig_prediction['boxes'][keep]\n    scores_prediction['scores'] = orig_prediction['scores'][keep]\n    scores_prediction['labels'] = orig_prediction['labels'][keep]\n    \n    keep = torchvision.ops.nms(scores_prediction['boxes'], scores_prediction['scores'], iou_thresh)\n    \n    final_prediction = {}\n    final_prediction['boxes'] = scores_prediction['boxes'][keep]\n    final_prediction['scores'] = scores_prediction['scores'][keep]\n    final_prediction['labels'] = scores_prediction['labels'][keep]\n    \n    return final_prediction\n\ndef compute_iou(bbox1, bbox2):\n    \"\"\"\n    Compute the IoU between two bounding boxes.\n\n    Parameters\n    ----------\n    - bbox1 (List[int]): Bounding box 1.\n    - bbox2 (List[int]): Bounding box 2.\n\n    Returns\n    -iou (float): Intersection over union\n    \"\"\"\n\n    # Compute the area for box1 and box2\n    area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n    area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n\n    # Compute the area of the intersection\n    xi1 = max(bbox1[0], bbox2[0])\n    xi2 = min(bbox1[2], bbox2[2])\n    yi1 = max(bbox1[1], bbox2[1])\n    yi2 = min(bbox1[3], bbox2[3])\n    intersection = max(xi2 - xi1, 0) * max(yi2 - yi1, 0)\n\n    # Compute the area of the union\n    union = area1 + area2 - intersection\n\n    return intersection / union\n\ndef cross_match(targets, predictions, iou_thresh=0.3, score_thresh=0.35, match_iou=0.5):\n    \"\"\"Cross mathc few targets\n   \n    Parameters\n    ----------\n    target : dictionary\n        Ground truth (correct) target boxes.\n    predictions : dictionary\n        Predicted boxes\n    iou_thresh : float\n        Intersection over union threshold for cross matching\n    score_thresh:\n        Use boxes with score greater than this threshold\n    match_iou:\n        Consider a match is iou is greater than this value\n    Returns\n    -------\n    match (true positive), missed (false negative), wrong (false positive)\n    \"\"\"\n    # Drop prections with lower cores\n    predictions = apply_nms(predictions, iou_thresh=iou_thresh, score_thresh=score_thresh)\n    \n    total_true = len(targets[\"boxes\"])\n    total_pred = len(predictions['boxes']) \n    \n    match = 0\n    missed = 0\n    wrong = 0\n\n    \n    if total_true == 0:\n        wrong = len(predictions['boxes'])\n        return predictions, [match, missed, wrong]\n    elif total_pred == 0:\n        missed = total_true\n        return predictions, [match, missed, wrong]\n    else:\n        for i in range(len(predictions['boxes'])):\n            box = predictions['boxes'][i].cpu().detach().numpy()\n            \n            try:\n                for j in range(len(targets['boxes'])):\n                    box1 = targets['boxes'][j].cpu().detach().numpy()\n                    iou = compute_iou(box, box1)\n                    if iou >= match_iou:\n                        match += 1\n            except:\n                print(targets['boxes'])\n            \n    missed = total_true - match\n    wrong = total_pred - match\n    \n    return predictions, [match, missed, wrong]\n                    \ndef preprocess_img(img):\n    img = img/255.\n    x,y, c = img.shape\n    img = img.transpose(2, 0, 1)\n    return torch.from_numpy(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Looking at some predictions","metadata":{}},{"cell_type":"code","source":"# train only on images with detections let see\n# add the imaging paths to the dataframe\ndf = pd.read_csv(\"../input/tensorflow-great-barrier-reef/train.csv\")\ndf['path'] = [f\"../input/tensorflow-great-barrier-reef/train_images/video_{a}/{b}.jpg\" for a,b in zip(df[\"video_id\"],df[\"video_frame\"])]\ndf['annotations'] = df['annotations'].apply(eval)\ndf['number_boxes'] = df['annotations'].apply(lambda x: len(x))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let resplit and test this on the on validation dataset","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:14:19.477527Z","iopub.execute_input":"2022-02-08T05:14:19.478213Z","iopub.status.idle":"2022-02-08T05:14:19.484898Z","shell.execute_reply.started":"2022-02-08T05:14:19.478174Z","shell.execute_reply":"2022-02-08T05:14:19.484154Z"}}},{"cell_type":"code","source":"val_df = get_train_val(df, train=False).reset_index(drop=True)\nval_ds = DataAdaptor(val_df)\ndataset_test = CotsData(val_df, get_transform(train=False))\ntotal_images = (len(val_df))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = []\ni = 0 \nfor img, target in dataset_test:\n    prediction = model([img.to(device)])[0]\n    pred, output = cross_match(target, prediction, iou_thresh=0.3, score_thresh=0.3, match_iou=0.5)\n    results.append(output)\n    \n    img = img.cpu().detach().numpy()\n    img = img.squeeze().transpose(1, 2, 0)\n    \n    bboxes = target['boxes'].cpu().detach().numpy()\n    pboxes = pred['boxes'].cpu().detach().numpy()\n    \n    if i%60 ==0:\n        val_ds.show_overlay(img, bboxes, pboxes, output)\n    \n    i+=1\n    if i==total_images:\n        break\n\nresults = np.asarray(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nplt.style.use('ggplot')\n\nmpl.rcParams['lines.linewidth'] = 2\nplt.rcParams[\"figure.figsize\"] = (20,16)\n\nn_images, _ = results.shape\nxval = np.array(range(1, n_images+1))\n\n@np.vectorize\ndef fb_score(beta, tp, fn, fp):\n    \"\"\"Compute the f-beta score for each image\n    \n    Parameters:\n    -b (float) - beta parameter to use\n    -tp (int) - numpber of true positives\n    -fn (int) - number of false negatives\n    -fp (int) - number of false positive\n    \n    Returns:\n    -fbeta (float) - fbeta score\n    \"\"\"\n    \n    if (tp == 0) and (fp==0) and (fn==0):\n        return np.nan\n    else:\n        fb = ((1.+beta**2)*(tp))/((1.+beta**2)*(tp) + (beta**2.)*fn + fp)\n    \n    return fb\n\nf2_scores = fb_score(2., results[:,0], results[:,1], results[:,2])\n\nplt.plot(xval, results[:,0], label=\"True positve\", color='b', linewidth=2, alpha=0.5)\nplt.plot(xval, results[:,0], marker='x', markersize=3, color='b', alpha=0.5, linestyle='None')\n\nplt.plot(xval, results[:,1], label=\"False negative\", color='r', linewidth=2, alpha=0.5)\nplt.plot(xval, results[:,1], marker='x', markersize=3, color='r', alpha=0.5, linestyle='None')\n\nplt.plot(xval, results[:,2], label=\"False positive\", color='g', linewidth=2, alpha=0.5)\nplt.plot(xval, results[:,2], marker='x', markersize=3, color='g', alpha=0.5, linestyle='None')\n\nplt.legend(loc='best',fontsize = 'large',numpoints=1, ncol=1)\nplt.ylabel(\"Stats\", size=20)\nplt.xlabel(\"Test image index\", size=20)\n# plt.title(title, size=15)\n\nplt.tight_layout()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(xval, f2_scores, label=f\"mean = {np.nanmean(f2_scores)}\", color='b', linewidth=2, alpha=0.5)\nplt.plot(xval, f2_scores, marker='x', markersize=3, color='b', alpha=0.5, linestyle='None')\n\nplt.legend(loc='best',fontsize = 'large',numpoints=1, ncol=1)\nplt.ylabel(\"F2 scores\", size=20)\nplt.xlabel(\"Test image index\", size=20)\n# plt.title(title, size=15)\n\nplt.tight_layout()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The images above shows true positives, false positves, false negatives, and F2 scores obtained for evey image in validation dataset. Here we used an IoU threshold of 0.5 to cross match the sources. We also discard predictions with scores below 0.3 as unreliable. We get an a score 0.6 using our metric. Although this score is not exactly the competition metric this gives a good indication that our model needs to improve drastically. ","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\nWe were able to fine-tune and train an object dection network on the COTS dataset. Our model gives an F2-score of 0.6 using IoU threshold of 0.5 on our validation dataset. This score is quite low but it is goot initial starting score for a baseline model. Also through the process, we have all improved our knowlegde of computer vision and object detection. Also given that we using a pretrained model we had little flexbility on tweaking certain parameters.","metadata":{}},{"cell_type":"markdown","source":"## Recommendations for future work\n\nSince this was a time-intensive project of about 2 weeks with most team-members performing their first computer vision + object detection + deep learning fine- tuning + Kaggle competition, there is definitely room for improvement. The following ideas were considered as potential further ameliorations, but could not be concretised given the lack of time:\n- YOLO\n- Hyperparameter fine-tuning\nâ€‹\nâ€‹\n\nYOLO (You Only Look Once) is a fast algorithm that recognizes objects within an image. It is a single CNN and requires only one forward pass through the neural network in order to identify the objects, which makes optimisation easier. It is different from the more tradional object detection algorithms which are classification based, where as YOLO is regression based. \nâ€‹\nHyperparameter fine-tuning is another approach that can be investigated into more depths next time. Currently, for the optimiser, an SGD is being implemented. However, the effect of varying its underlyling base parameters were not explored, such as the lr (learning rate), momentum, and weight decay. Moreover, a base step learning rate scheduler was also implemented and its parameters such as the step_size and gamma were kept default. Note, the step learning rate decays the learning rate of each parameter group every step_size epochs by the gamma value. Since we were training our models on Kaggle, and had limited GPU quota of 30 hours per week, we were not able to dedicate as much time to hyperparameter fine-tuning as expected. In the second week of the DSI, an AWS elastic computing cloud (ec2) was made available but we were having difficulties running the training algorithm on it. \n\nAnother important observation is how small the starfishes appear in the images. So another recommendation for futurework is to find the right augmentation that can address this issue.","metadata":{}},{"cell_type":"markdown","source":"## References \n[1] PyTorch, \"Torchvision Object Detection Finetuning tutorial\". URL: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html. Accessed on: 25/01/22\n\n[2] Passos, A., \"Be CAREFUL with your TRAIN/VALID splits and avoid LB GAPS\". URL: https://www.kaggle.com/c/tensorflow-great-barrier-reef/discussion/293723. Accessed on: 02/02/22 \n\n[3] Jiajun, L. et al., \"The CSIRO Crown-of-Thorn Starfish Detection Dataset\", 2021\n\n[4] Kaggle notebook. URL: https://www.kaggle.com/mrinath/efficientdet-train-pytorch","metadata":{}}]}