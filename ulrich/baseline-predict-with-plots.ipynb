{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This preliminary notebook using some code form https://www.kaggle.com/mrinath/efficientdet-train-pytorch","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/input/k/ulricharmel/baseline-predict-pytorch","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:06.069218Z","iopub.execute_input":"2022-02-08T05:35:06.069592Z","iopub.status.idle":"2022-02-08T05:35:07.044883Z","shell.execute_reply.started":"2022-02-08T05:35:06.069550Z","shell.execute_reply":"2022-02-08T05:35:07.043799Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"!cp /kaggle/input/k/ulricharmel/baseline-predict-pytorch/utils.py /kaggle/working/\n!cp /kaggle/input/k/ulricharmel/baseline-predict-pytorch/transforms.py /kaggle/working/\n!cp /kaggle/input/k/ulricharmel/baseline-predict-pytorch/coco_eval.py /kaggle/working/\n!cp /kaggle/input/k/ulricharmel/baseline-predict-pytorch/engine.py /kaggle/working/\n!cp /kaggle/input/k/ulricharmel/baseline-predict-pytorch/coco_utils.py /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:07.054796Z","iopub.execute_input":"2022-02-08T05:35:07.055374Z","iopub.status.idle":"2022-02-08T05:35:10.822834Z","shell.execute_reply.started":"2022-02-08T05:35:07.055324Z","shell.execute_reply":"2022-02-08T05:35:10.821714Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# plot some of the images\nimport cv2\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches\n\ndef get_rectangle_edges_from_pascal_bbox(bbox):\n    xmin_top_left, ymin_top_left, xmax_bottom_right, ymax_bottom_right = bbox\n\n    bottom_left = (xmin_top_left, ymax_bottom_right)\n    width = xmax_bottom_right - xmin_top_left\n    height = ymin_top_left - ymax_bottom_right\n\n    return bottom_left, width, height\n\ndef draw_pascal_voc_bboxes(\n    plot_ax,\n    bboxes,\n    get_rectangle_corners_fn=get_rectangle_edges_from_pascal_bbox,\n    color='red'\n):\n    for bbox in bboxes:\n        bottom_left, width, height = get_rectangle_corners_fn(bbox)\n\n        rect_1 = patches.Rectangle(\n            bottom_left,\n            width,\n            height,\n            linewidth=2,\n            edgecolor=\"black\",\n            fill=False,\n        )\n        rect_2 = patches.Rectangle(\n            bottom_left,\n            width,\n            height,\n            linewidth=2,\n            edgecolor=color,\n            fill=False,\n        )\n\n        # Add the patch to the Axes\n        plot_ax.add_patch(rect_1)\n        plot_ax.add_patch(rect_2)\n\ndef draw_image(\n    image, bboxes=None, pboxes=None, image_id=None, color2='blue', draw_bboxes_fn=draw_pascal_voc_bboxes, figsize=(10, 10)\n):\n    fig, ax = plt.subplots(1, figsize=figsize)\n    ax.imshow(image)\n    if image_id is not None:\n        ax.set_title(f\"{image_id}\")\n\n    if bboxes is not None:\n        draw_bboxes_fn(ax, bboxes, color='red')\n    if pboxes is not None:\n        draw_bboxes_fn(ax, pboxes, color=color2)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:10.824707Z","iopub.execute_input":"2022-02-08T05:35:10.825045Z","iopub.status.idle":"2022-02-08T05:35:10.839570Z","shell.execute_reply.started":"2022-02-08T05:35:10.825000Z","shell.execute_reply":"2022-02-08T05:35:10.838648Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"class DataAdaptor:\n    def __init__(self,df):\n        self.df = df\n    def __len__(self):\n        return len(self.df)\n    \n    def get_boxes(self, row):\n        \"\"\"Returns the bboxes for a given row as a 3D matrix with format [x_min, y_min, x_max, y_max]\"\"\"\n        \n        boxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\n        \n        # Change from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n        boxes[:, 2] = np.clip(boxes[:, 0] + boxes[:, 2],0,1280)\n        boxes[:, 3] = np.clip(boxes[:, 1] + boxes[:, 3],0,720) \n        \n        return boxes\n    \n    def get_image_bb(self , idx):\n        img_src = self.df.loc[idx,'path']\n        image   = cv2.imread(img_src)\n        image   = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        row     = self.df.iloc[idx]\n        bboxes  = self.get_boxes(row) \n        class_labels = np.ones(len(bboxes))\n        return image, bboxes, class_labels, idx\n    \n    def show_overlay(self, image, boxes, pboxes, image_id, color2=\"blue\"):\n        draw_image(image, bboxes.tolist(), pboxes.tolist(), image_id=image_id, color2=color2)\n        \n    def show_image(self, index):\n        image, bboxes, class_labels, image_id = self.get_image_bb(index)\n        print(f\"image_id: {image_id}\")\n        draw_image(image, bboxes.tolist())\n#         print(class_labels) \n        return image","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:10.841342Z","iopub.execute_input":"2022-02-08T05:35:10.841947Z","iopub.status.idle":"2022-02-08T05:35:10.855605Z","shell.execute_reply.started":"2022-02-08T05:35:10.841904Z","shell.execute_reply":"2022-02-08T05:35:10.854852Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.utils.data\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch import optim\nfrom torchvision import transforms\n\n\nclass CotsData(torch.utils.data.Dataset):\n    def __init__(self, df, transforms=None):\n        self.ds = df\n        self.transforms = transforms\n    \n    def get_boxes(self, row):\n        \"\"\"Returns the bboxes for a given row as a 3D matrix with format [x_min, y_min, x_max, y_max]\"\"\"\n        \n        boxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\n        \n        # Change from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n        boxes[:, 2] = np.clip(boxes[:, 0] + boxes[:, 2],0,1280)\n        boxes[:, 3] = np.clip(boxes[:, 1] + boxes[:, 3],0,720) \n        \n        return boxes\n            \n    def __getitem__(self, idx):\n        # load images\n        img_path = self.ds.loc[idx,'path']\n        # mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        # note that we haven't converted the mask to RGB,\n        \n        row = self.ds.iloc[idx]\n        boxes = self.get_boxes(row)\n        num_objs = self.ds.loc[idx, 'number_boxes']\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        \n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64) # check this probably have to set this to true\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.ds)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:10.858439Z","iopub.execute_input":"2022-02-08T05:35:10.858723Z","iopub.status.idle":"2022-02-08T05:35:10.874970Z","shell.execute_reply.started":"2022-02-08T05:35:10.858672Z","shell.execute_reply":"2022-02-08T05:35:10.874144Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n      \ndef get_instance_segmentation_model(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n#     model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False) #pretrained=True\n    model.load_state_dict(torch.load('/kaggle/input/resnet/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth'))\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n#     # now get the number of input features for the mask classifier\n#     in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n#     hidden_layer = 256\n#     # and replace the mask predictor with a new one\n#     model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n#                                                        hidden_layer,\n#                                                        num_classes)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:10.877388Z","iopub.execute_input":"2022-02-08T05:35:10.877569Z","iopub.status.idle":"2022-02-08T05:35:10.888839Z","shell.execute_reply.started":"2022-02-08T05:35:10.877547Z","shell.execute_reply":"2022-02-08T05:35:10.888166Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# from engine import train_one_epoch, evaluate\n# import utils\nimport transforms as T\n\n\ndef get_transform(train):\n    transforms = []\n    # converts the image, a PIL image, into a PyTorch Tensor\n    transforms.append(T.ToTensor())\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:10.890205Z","iopub.execute_input":"2022-02-08T05:35:10.890609Z","iopub.status.idle":"2022-02-08T05:35:10.901058Z","shell.execute_reply.started":"2022-02-08T05:35:10.890573Z","shell.execute_reply":"2022-02-08T05:35:10.900357Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# we redifine the model\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n\n# get the model using our helper function\nmodel = get_instance_segmentation_model(num_classes)\n# move model to the right device\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:10.902349Z","iopub.execute_input":"2022-02-08T05:35:10.902621Z","iopub.status.idle":"2022-02-08T05:35:11.670203Z","shell.execute_reply.started":"2022-02-08T05:35:10.902587Z","shell.execute_reply":"2022-02-08T05:35:11.669492Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"\n# This loads the model\nmodel_path = '/kaggle/input/checkvideo2/checkpoint-video2.pth'\nstate_dict = torch.load(model_path)\n# print(state_dict.keys())\nmodel.load_state_dict(state_dict)\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:11.671673Z","iopub.execute_input":"2022-02-08T05:35:11.672170Z","iopub.status.idle":"2022-02-08T05:35:11.818209Z","shell.execute_reply.started":"2022-02-08T05:35:11.672133Z","shell.execute_reply":"2022-02-08T05:35:11.817547Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"def apply_nms(orig_prediction, iou_thresh=0.3, score_thresh=0.35):\n    \n    # torchvision returns the indices of the bboxes to keep\n    # function to implement non maximm suppression\n    # might also need to eliminate predictions with very low scores\n    # trim low scores first\n    \n    keep = orig_prediction['scores'] >= score_thresh\n    \n    scores_prediction = {}\n    scores_prediction['boxes'] = orig_prediction['boxes'][keep]\n    scores_prediction['scores'] = orig_prediction['scores'][keep]\n    scores_prediction['labels'] = orig_prediction['labels'][keep]\n    \n    keep = torchvision.ops.nms(scores_prediction['boxes'], scores_prediction['scores'], iou_thresh)\n    \n    final_prediction = {}\n    final_prediction['boxes'] = scores_prediction['boxes'][keep]\n    final_prediction['scores'] = scores_prediction['scores'][keep]\n    final_prediction['labels'] = scores_prediction['labels'][keep]\n    \n    return final_prediction\n\ndef return_predict_string(predictions):\n    str_p = ''\n    for i, score in enumerate(predictions['scores']):\n        box = predictions['boxes'][i].cpu().detach().numpy()\n        score = score.cpu().detach().numpy()\n        str_p += f'{score:.3f} {int(np.round(box[0]))} {int(np.round(box[1]))} {int(np.round(box[2]-box[0]))} {int(np.round(box[3]-box[1]))} '\n    \n    str_p = str_p.strip(' ')\n#     if str_p == '':\n#         str_p = '0.9 716 678 54 42'\n    \n    return str_p\n\ndef preprocess_img(img):\n    img = img/255.\n    x,y, c = img.shape\n    img = img.transpose(2, 0, 1)\n    return torch.from_numpy(img)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:11.819402Z","iopub.execute_input":"2022-02-08T05:35:11.819741Z","iopub.status.idle":"2022-02-08T05:35:11.830367Z","shell.execute_reply.started":"2022-02-08T05:35:11.819703Z","shell.execute_reply":"2022-02-08T05:35:11.829724Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"### looking at some precitions","metadata":{}},{"cell_type":"code","source":"# train only on images with detections let see\ndf = pd.read_csv(\"../input/tensorflow-great-barrier-reef/train.csv\")\n# df = df[df.annotations != '[]']\n# df = df.reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:11.831732Z","iopub.execute_input":"2022-02-08T05:35:11.832186Z","iopub.status.idle":"2022-02-08T05:35:11.870182Z","shell.execute_reply.started":"2022-02-08T05:35:11.832148Z","shell.execute_reply":"2022-02-08T05:35:11.869550Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# add the imaging paths to the dataframe\ndf['path'] = [f\"../input/tensorflow-great-barrier-reef/train_images/video_{a}/{b}.jpg\" for a,b in zip(df[\"video_id\"],df[\"video_frame\"])]\ndf['annotations'] = df['annotations'].apply(eval)\ndf['number_boxes'] = df['annotations'].apply(lambda x: len(x))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:11.871651Z","iopub.execute_input":"2022-02-08T05:35:11.872129Z","iopub.status.idle":"2022-02-08T05:35:12.265835Z","shell.execute_reply.started":"2022-02-08T05:35:11.872092Z","shell.execute_reply":"2022-02-08T05:35:12.265117Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"np.where(df[\"number_boxes\"] > 2)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:12.267012Z","iopub.execute_input":"2022-02-08T05:35:12.267251Z","iopub.status.idle":"2022-02-08T05:35:12.275325Z","shell.execute_reply.started":"2022-02-08T05:35:12.267216Z","shell.execute_reply":"2022-02-08T05:35:12.274492Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"train_ds = DataAdaptor(df)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:12.279382Z","iopub.execute_input":"2022-02-08T05:35:12.279566Z","iopub.status.idle":"2022-02-08T05:35:12.285802Z","shell.execute_reply.started":"2022-02-08T05:35:12.279544Z","shell.execute_reply":"2022-02-08T05:35:12.284972Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"image, bboxes, _, image_id = train_ds.get_image_bb(2627)\npixel_p = preprocess_img(image)\nprediction = model([pixel_p.to(device, dtype=torch.float)])[0]\npboxes = prediction['boxes'].cpu().detach().numpy()\n\n# now call the imaging function to overlay the boxes\ntrain_ds.show_overlay(image, bboxes, pboxes, image_id)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:12.287020Z","iopub.execute_input":"2022-02-08T05:35:12.287656Z","iopub.status.idle":"2022-02-08T05:35:12.817205Z","shell.execute_reply.started":"2022-02-08T05:35:12.287619Z","shell.execute_reply":"2022-02-08T05:35:12.816537Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"### let replit and test this on the on validation dataset","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:14:19.477527Z","iopub.execute_input":"2022-02-08T05:14:19.478213Z","iopub.status.idle":"2022-02-08T05:14:19.484898Z","shell.execute_reply.started":"2022-02-08T05:14:19.478174Z","shell.execute_reply":"2022-02-08T05:14:19.484154Z"}}},{"cell_type":"code","source":"# get training and validation dataframes\ndef get_train_val(df, train=True):\n    if train:\n        df2 = df[df.video_id != 2]\n        dfn = df2[df.number_boxes>0]\n        dfo = df2[df.number_boxes==0]\n        dfno = dfo.sample(n=1000, replace=False, random_state=1)\n        result = pd.concat([dfn, dfno])\n    else:\n        df2 = df[df.video_id==2]\n        dfn = df2[df.number_boxes>0]\n        dfo = df2[df.number_boxes==0]\n        dfno = dfo.sample(n=100, replace=False, random_state=1)\n        result = pd.concat([dfn, dfno])\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:12.818493Z","iopub.execute_input":"2022-02-08T05:35:12.818884Z","iopub.status.idle":"2022-02-08T05:35:12.826636Z","shell.execute_reply.started":"2022-02-08T05:35:12.818845Z","shell.execute_reply":"2022-02-08T05:35:12.825972Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"val_df = get_train_val(df, train=False)\nval_ds = DataAdaptor(val_df.reset_index(drop=True))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:12.827871Z","iopub.execute_input":"2022-02-08T05:35:12.828251Z","iopub.status.idle":"2022-02-08T05:35:12.848893Z","shell.execute_reply.started":"2022-02-08T05:35:12.828213Z","shell.execute_reply":"2022-02-08T05:35:12.847929Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"np.where(val_df[\"number_boxes\"] > 2)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:12.850267Z","iopub.execute_input":"2022-02-08T05:35:12.850622Z","iopub.status.idle":"2022-02-08T05:35:12.860336Z","shell.execute_reply.started":"2022-02-08T05:35:12.850582Z","shell.execute_reply":"2022-02-08T05:35:12.859335Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"image, bboxes, _, image_id = val_ds.get_image_bb(29)\npixel_p = preprocess_img(image)\nprediction = model([pixel_p.to(device, dtype=torch.float)])[0]\npboxes = prediction['boxes'].cpu().detach().numpy()\n\n# now call the imaging function to overlay the boxes\ntrain_ds.show_overlay(image, bboxes, pboxes, image_id)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:12.861709Z","iopub.execute_input":"2022-02-08T05:35:12.862540Z","iopub.status.idle":"2022-02-08T05:35:13.363431Z","shell.execute_reply.started":"2022-02-08T05:35:12.862501Z","shell.execute_reply":"2022-02-08T05:35:13.362756Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"# prediction","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:13.427158Z","iopub.execute_input":"2022-02-08T05:35:13.427342Z","iopub.status.idle":"2022-02-08T05:35:13.433388Z","shell.execute_reply.started":"2022-02-08T05:35:13.427313Z","shell.execute_reply":"2022-02-08T05:35:13.432628Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"# pixel_p.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:13.434292Z","iopub.execute_input":"2022-02-08T05:35:13.434773Z","iopub.status.idle":"2022-02-08T05:35:13.441712Z","shell.execute_reply.started":"2022-02-08T05:35:13.434736Z","shell.execute_reply":"2022-02-08T05:35:13.441007Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"# pixel_p*255.","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:13.443613Z","iopub.execute_input":"2022-02-08T05:35:13.443996Z","iopub.status.idle":"2022-02-08T05:35:13.451471Z","shell.execute_reply.started":"2022-02-08T05:35:13.443965Z","shell.execute_reply":"2022-02-08T05:35:13.450567Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"# plt.imshow(pixel_p.reshape(720,1280,3))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:35:13.452865Z","iopub.execute_input":"2022-02-08T05:35:13.453397Z","iopub.status.idle":"2022-02-08T05:35:13.460013Z","shell.execute_reply.started":"2022-02-08T05:35:13.453359Z","shell.execute_reply":"2022-02-08T05:35:13.459334Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}