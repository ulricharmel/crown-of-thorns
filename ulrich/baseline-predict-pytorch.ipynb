{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This preliminary notebook using some code form https://www.kaggle.com/mrinath/efficientdet-train-pytorch","metadata":{}},{"cell_type":"code","source":"# Try doing all the installations here\n!pip install -I numpy\n\n!pip install -I torchvision\n!pip install -I torch -U   \n\n# First, we need to install pycocotools. This library will be used for computing the evaluation metrics following the COCO metric for intersection over union.\n\n!pip install cython\n# Install pycocotools, the version by default in Colab\n# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n!pip install -I 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI' --no-binary pycocotools\n!ls /kaggle/input/baseline-predict-pytorch\n\n# # !pip install -I pycocotools==2.0.0\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:13:41.830974Z","iopub.execute_input":"2022-02-02T17:13:41.831453Z","iopub.status.idle":"2022-02-02T17:16:36.11045Z","shell.execute_reply.started":"2022-02-02T17:13:41.83131Z","shell.execute_reply":"2022-02-02T17:16:36.109633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !ls /kaggle/input/baseline-predict-pytorch","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:16:36.112576Z","iopub.execute_input":"2022-02-02T17:16:36.112933Z","iopub.status.idle":"2022-02-02T17:16:36.120575Z","shell.execute_reply.started":"2022-02-02T17:16:36.112892Z","shell.execute_reply":"2022-02-02T17:16:36.119905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !cp -r /kaggle/input/cococode/PythonAPI/pycocotools/ /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:16:36.121759Z","iopub.execute_input":"2022-02-02T17:16:36.123136Z","iopub.status.idle":"2022-02-02T17:16:36.142448Z","shell.execute_reply.started":"2022-02-02T17:16:36.123097Z","shell.execute_reply":"2022-02-02T17:16:36.141661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !python /kaggle/input/cococode/PythonAPI/setup.py  build_ext install","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:16:36.144532Z","iopub.execute_input":"2022-02-02T17:16:36.144711Z","iopub.status.idle":"2022-02-02T17:16:36.151308Z","shell.execute_reply.started":"2022-02-02T17:16:36.144688Z","shell.execute_reply":"2022-02-02T17:16:36.150476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pycocotools._mask as mask","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:16:36.152762Z","iopub.execute_input":"2022-02-02T17:16:36.153326Z","iopub.status.idle":"2022-02-02T17:16:36.158844Z","shell.execute_reply.started":"2022-02-02T17:16:36.15329Z","shell.execute_reply":"2022-02-02T17:16:36.158138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# git clone the utility functions and evaluation functions from pytorch coco dataset\n\n!git clone https://github.com/pytorch/vision.git\n!cd vision\n!git checkout v0.8.2","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:16:36.160227Z","iopub.execute_input":"2022-02-02T17:16:36.160472Z","iopub.status.idle":"2022-02-02T17:16:57.121599Z","shell.execute_reply.started":"2022-02-02T17:16:36.16044Z","shell.execute_reply":"2022-02-02T17:16:57.120743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls vision/references","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:16:57.124516Z","iopub.execute_input":"2022-02-02T17:16:57.124845Z","iopub.status.idle":"2022-02-02T17:16:57.83081Z","shell.execute_reply.started":"2022-02-02T17:16:57.124806Z","shell.execute_reply":"2022-02-02T17:16:57.829968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp vision/references/detection/utils.py /kaggle/working/\n!cp vision/references/detection/transforms.py /kaggle/working/\n!cp vision/references/detection/coco_eval.py /kaggle/working/\n!cp vision/references/detection/engine.py /kaggle/working/\n!cp vision/references/detection/coco_utils.py /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:16:57.832838Z","iopub.execute_input":"2022-02-02T17:16:57.833126Z","iopub.status.idle":"2022-02-02T17:17:01.353956Z","shell.execute_reply.started":"2022-02-02T17:16:57.833086Z","shell.execute_reply":"2022-02-02T17:17:01.353021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport ast\nimport sys\nimport os\n\n# os.environ['TORCH_HOME'] = '\\\\kaggle\\\\input\\\\resnet'\n\nimport glob\nimport sklearn\nimport math\nimport random\n\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom transformers import get_cosine_schedule_with_warmup\n\nfrom PIL import Image\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn import metrics, model_selection, preprocessing\nfrom sklearn.model_selection import GroupKFold\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(device)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-02T17:17:01.355838Z","iopub.execute_input":"2022-02-02T17:17:01.356199Z","iopub.status.idle":"2022-02-02T17:17:08.575191Z","shell.execute_reply.started":"2022-02-02T17:17:01.356149Z","shell.execute_reply":"2022-02-02T17:17:08.574298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train only on images with detections let see\ndf = pd.read_csv(\"../input/tensorflow-great-barrier-reef/train.csv\")\n# df = df[df.annotations != '[]']\n# df = df.reset_index(drop = True)\ndf.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:08.578933Z","iopub.execute_input":"2022-02-02T17:17:08.579191Z","iopub.status.idle":"2022-02-02T17:17:08.659227Z","shell.execute_reply.started":"2022-02-02T17:17:08.57916Z","shell.execute_reply":"2022-02-02T17:17:08.658214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['fold'] = -1\nkf = GroupKFold(n_splits = 5)\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df, y = df.video_id.tolist(), groups=df.sequence)):\n    df.loc[val_idx, 'fold'] = fold","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:08.660947Z","iopub.execute_input":"2022-02-02T17:17:08.661281Z","iopub.status.idle":"2022-02-02T17:17:08.681502Z","shell.execute_reply.started":"2022-02-02T17:17:08.661241Z","shell.execute_reply":"2022-02-02T17:17:08.680667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:08.682863Z","iopub.execute_input":"2022-02-02T17:17:08.683262Z","iopub.status.idle":"2022-02-02T17:17:08.697137Z","shell.execute_reply.started":"2022-02-02T17:17:08.683219Z","shell.execute_reply":"2022-02-02T17:17:08.696137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.fold.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:08.699044Z","iopub.execute_input":"2022-02-02T17:17:08.699393Z","iopub.status.idle":"2022-02-02T17:17:08.710521Z","shell.execute_reply.started":"2022-02-02T17:17:08.699352Z","shell.execute_reply":"2022-02-02T17:17:08.709637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add the imaging paths to the dataframe\ndf['path'] = [f\"../input/tensorflow-great-barrier-reef/train_images/video_{a}/{b}.jpg\" for a,b in zip(df[\"video_id\"],df[\"video_frame\"])]\ndf['annotations'] = df['annotations'].apply(eval)\ndf['number_boxes'] = df['annotations'].apply(lambda x: len(x))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:08.712146Z","iopub.execute_input":"2022-02-02T17:17:08.713211Z","iopub.status.idle":"2022-02-02T17:17:09.171652Z","shell.execute_reply.started":"2022-02-02T17:17:08.712994Z","shell.execute_reply":"2022-02-02T17:17:09.170858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot some of the images\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches\n\ndef get_rectangle_edges_from_pascal_bbox(bbox):\n    xmin_top_left, ymin_top_left, xmax_bottom_right, ymax_bottom_right = bbox\n\n    bottom_left = (xmin_top_left, ymax_bottom_right)\n    width = xmax_bottom_right - xmin_top_left\n    height = ymin_top_left - ymax_bottom_right\n\n    return bottom_left, width, height\n\ndef draw_pascal_voc_bboxes(\n    plot_ax,\n    bboxes,\n    get_rectangle_corners_fn=get_rectangle_edges_from_pascal_bbox,\n):\n    for bbox in bboxes:\n        bottom_left, width, height = get_rectangle_corners_fn(bbox)\n\n        rect_1 = patches.Rectangle(\n            bottom_left,\n            width,\n            height,\n            linewidth=2,\n            edgecolor=\"black\",\n            fill=False,\n        )\n        rect_2 = patches.Rectangle(\n            bottom_left,\n            width,\n            height,\n            linewidth=2,\n            edgecolor=\"red\",\n            fill=False,\n        )\n\n        # Add the patch to the Axes\n        plot_ax.add_patch(rect_1)\n        plot_ax.add_patch(rect_2)\n\ndef draw_image(\n    image, bboxes=None, draw_bboxes_fn=draw_pascal_voc_bboxes, figsize=(10, 10)\n):\n    fig, ax = plt.subplots(1, figsize=figsize)\n    ax.imshow(image)\n\n    if bboxes is not None:\n        draw_bboxes_fn(ax, bboxes)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:09.172984Z","iopub.execute_input":"2022-02-02T17:17:09.173323Z","iopub.status.idle":"2022-02-02T17:17:09.186068Z","shell.execute_reply.started":"2022-02-02T17:17:09.173285Z","shell.execute_reply":"2022-02-02T17:17:09.185296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataAdaptor:\n    def __init__(self,df):\n        self.df = df\n    def __len__(self):\n        return len(self.df)\n    \n    def get_boxes(self, row):\n        \"\"\"Returns the bboxes for a given row as a 3D matrix with format [x_min, y_min, x_max, y_max]\"\"\"\n        \n        boxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\n        \n        # Change from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n        boxes[:, 2] = np.clip(boxes[:, 0] + boxes[:, 2],0,1280)\n        boxes[:, 3] = np.clip(boxes[:, 1] + boxes[:, 3],0,720) \n        \n        return boxes\n    \n    def get_image_bb(self , idx):\n        img_src = self.df.loc[idx,'path']\n        image   = cv2.imread(img_src)\n        image   = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        row     = self.df.iloc[idx]\n        bboxes  = self.get_boxes(row) \n        class_labels = np.ones(len(bboxes))\n        return image, bboxes, class_labels, idx\n    \n        \n    def show_image(self, index):\n        image, bboxes, class_labels, image_id = self.get_image_bb(index)\n        print(f\"image_id: {image_id}\")\n        draw_image(image, bboxes.tolist())\n#         print(class_labels) \n        return image","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:09.187514Z","iopub.execute_input":"2022-02-02T17:17:09.187735Z","iopub.status.idle":"2022-02-02T17:17:09.201849Z","shell.execute_reply.started":"2022-02-02T17:17:09.187707Z","shell.execute_reply":"2022-02-02T17:17:09.201068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = DataAdaptor(df)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:09.203417Z","iopub.execute_input":"2022-02-02T17:17:09.204193Z","iopub.status.idle":"2022-02-02T17:17:09.213287Z","shell.execute_reply.started":"2022-02-02T17:17:09.204151Z","shell.execute_reply":"2022-02-02T17:17:09.212397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im,bb,_,_ = train_ds.get_image_bb(4005)\nbb","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:09.214583Z","iopub.execute_input":"2022-02-02T17:17:09.214923Z","iopub.status.idle":"2022-02-02T17:17:09.311724Z","shell.execute_reply.started":"2022-02-02T17:17:09.21489Z","shell.execute_reply":"2022-02-02T17:17:09.31092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = train_ds.show_image(2016)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:09.317496Z","iopub.execute_input":"2022-02-02T17:17:09.318316Z","iopub.status.idle":"2022-02-02T17:17:09.997957Z","shell.execute_reply.started":"2022-02-02T17:17:09.318276Z","shell.execute_reply":"2022-02-02T17:17:09.997086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.where(df[\"number_boxes\"] > 2)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:09.999727Z","iopub.execute_input":"2022-02-02T17:17:10.000317Z","iopub.status.idle":"2022-02-02T17:17:10.015514Z","shell.execute_reply.started":"2022-02-02T17:17:10.00028Z","shell.execute_reply":"2022-02-02T17:17:10.006583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_seq = [len(df[df['video_id'] == i]) for i in range(3)]\nlabels = [\"0\", \"1\", \"2\"]\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(9,6))\nax.set_facecolor('aliceblue')\nplt.grid(color=\"gray\", linestyle=\"-\", zorder=0)\nplt.ylabel(\"Number of Frames\", fontsize=16, fontweight=\"bold\")\nplt.xlabel(\"Video ID\", fontsize=16, fontweight=\"bold\")\nplt.title(\"Length of train videos\", fontsize=20, fontweight=\"bold\")\nplt.bar(labels, num_seq, color=\"orange\", zorder=3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:10.016428Z","iopub.execute_input":"2022-02-02T17:17:10.016663Z","iopub.status.idle":"2022-02-02T17:17:10.305765Z","shell.execute_reply.started":"2022-02-02T17:17:10.016634Z","shell.execute_reply":"2022-02-02T17:17:10.305148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_num = max(df.number_boxes)\nmax_sample = df[df[\"number_boxes\"] == max_num].sample()\nmax_vid_id = max_sample.video_id.values[0]\nmax_vid_frame = max_sample.video_frame.values[0]\n\nprint('\\033[1m' + f\"Maximum number of starfish in one frame: {max_num} (Video {max_vid_id}, Frame {max_vid_frame})\" + '\\033[0m')","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:10.309748Z","iopub.execute_input":"2022-02-02T17:17:10.310514Z","iopub.status.idle":"2022-02-02T17:17:10.330782Z","shell.execute_reply.started":"2022-02-02T17:17:10.310478Z","shell.execute_reply":"2022-02-02T17:17:10.329879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = train_ds.show_image(max_sample.index[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:10.335619Z","iopub.execute_input":"2022-02-02T17:17:10.336307Z","iopub.status.idle":"2022-02-02T17:17:10.856388Z","shell.execute_reply.started":"2022-02-02T17:17:10.336271Z","shell.execute_reply":"2022-02-02T17:17:10.854762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check number of samples without boxes\nmin_num = 0\nmin_sample = df[df[\"number_boxes\"] == 0]\nprint(len(min_sample), len(df), len(df)-len(min_sample))","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:10.857886Z","iopub.execute_input":"2022-02-02T17:17:10.858385Z","iopub.status.idle":"2022-02-02T17:17:10.867701Z","shell.execute_reply.started":"2022-02-02T17:17:10.85835Z","shell.execute_reply":"2022-02-02T17:17:10.866939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.utils.data\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch import optim\nfrom torchvision import transforms\n\n\nclass CotsData(torch.utils.data.Dataset):\n    def __init__(self, df, transforms=None):\n        self.ds = df\n        self.transforms = transforms\n    \n    def get_boxes(self, row):\n        \"\"\"Returns the bboxes for a given row as a 3D matrix with format [x_min, y_min, x_max, y_max]\"\"\"\n        \n        boxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\n        \n        # Change from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n        boxes[:, 2] = np.clip(boxes[:, 0] + boxes[:, 2],0,1280)\n        boxes[:, 3] = np.clip(boxes[:, 1] + boxes[:, 3],0,720) \n        \n        return boxes\n            \n    def __getitem__(self, idx):\n        # load images\n        img_path = self.ds.loc[idx,'path']\n        # mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        # note that we haven't converted the mask to RGB,\n        \n        row = self.ds.iloc[idx]\n        boxes = self.get_boxes(row)\n        num_objs = self.ds.loc[idx, 'number_boxes']\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        \n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64) # check this probably have to set this to true\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.ds)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:10.869336Z","iopub.execute_input":"2022-02-02T17:17:10.869885Z","iopub.status.idle":"2022-02-02T17:17:10.88925Z","shell.execute_reply.started":"2022-02-02T17:17:10.869846Z","shell.execute_reply":"2022-02-02T17:17:10.888368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n      \ndef get_instance_segmentation_model(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n#     model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) #pretrained=True trainable_backbone_layers=4\n#     model.load_state_dict(torch.load('/kaggle/input/resnet/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth'))\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n#     # now get the number of input features for the mask classifier\n#     in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n#     hidden_layer = 256\n#     # and replace the mask predictor with a new one\n#     model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n#                                                        hidden_layer,\n#                                                        num_classes)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:10.890333Z","iopub.execute_input":"2022-02-02T17:17:10.890526Z","iopub.status.idle":"2022-02-02T17:17:10.903153Z","shell.execute_reply.started":"2022-02-02T17:17:10.890504Z","shell.execute_reply":"2022-02-02T17:17:10.902405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input/tensorflow-great-barrier-reef","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:10.904689Z","iopub.execute_input":"2022-02-02T17:17:10.905032Z","iopub.status.idle":"2022-02-02T17:17:11.648065Z","shell.execute_reply.started":"2022-02-02T17:17:10.904983Z","shell.execute_reply":"2022-02-02T17:17:11.647131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_npy = np.load(\"/kaggle/input/tensorflow-great-barrier-reef/example_test.npy\")","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:11.65386Z","iopub.execute_input":"2022-02-02T17:17:11.654332Z","iopub.status.idle":"2022-02-02T17:17:11.791896Z","shell.execute_reply.started":"2022-02-02T17:17:11.654293Z","shell.execute_reply":"2022-02-02T17:17:11.791048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testdf = pd.read_csv(\"/kaggle/input/tensorflow-great-barrier-reef/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:11.793551Z","iopub.execute_input":"2022-02-02T17:17:11.793845Z","iopub.status.idle":"2022-02-02T17:17:11.805146Z","shell.execute_reply.started":"2022-02-02T17:17:11.793804Z","shell.execute_reply":"2022-02-02T17:17:11.804316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subdf = pd.read_csv(\"/kaggle/input/tensorflow-great-barrier-reef/example_sample_submission.csv\")\nsubdf","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:11.808205Z","iopub.execute_input":"2022-02-02T17:17:11.808436Z","iopub.status.idle":"2022-02-02T17:17:11.824603Z","shell.execute_reply.started":"2022-02-02T17:17:11.80841Z","shell.execute_reply":"2022-02-02T17:17:11.823892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat transforms.py","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:11.826871Z","iopub.execute_input":"2022-02-02T17:17:11.827153Z","iopub.status.idle":"2022-02-02T17:17:12.567299Z","shell.execute_reply.started":"2022-02-02T17:17:11.827122Z","shell.execute_reply":"2022-02-02T17:17:12.566394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input/tensorflow-great-barrier-reef/greatbarrierreef/","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:12.571047Z","iopub.execute_input":"2022-02-02T17:17:12.571276Z","iopub.status.idle":"2022-02-02T17:17:13.266706Z","shell.execute_reply.started":"2022-02-02T17:17:12.571247Z","shell.execute_reply":"2022-02-02T17:17:13.265872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from engine import train_one_epoch, evaluate\nimport utils\nimport transforms as T\n\n\ndef get_transform(train):\n    transforms = []\n    # converts the image, a PIL image, into a PyTorch Tensor\n    transforms.append(T.ToTensor())\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        transforms.append(T.RandomHorizontalFlip(0.5))\n#         transforms.append(T.RandomPhotometricDistort())\n#         transforms.append(T.RandomZoomOut())\n    return T.Compose(transforms)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:13.268431Z","iopub.execute_input":"2022-02-02T17:17:13.268921Z","iopub.status.idle":"2022-02-02T17:17:13.294687Z","shell.execute_reply.started":"2022-02-02T17:17:13.26887Z","shell.execute_reply":"2022-02-02T17:17:13.294018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get training and validation dataframes\ndef get_train_val(df, train=True):\n    if train:\n        df2 = df[df.video_id != 2]\n        dfn = df2[df.number_boxes>0]\n        dfo = df2[df.number_boxes==0]\n        dfno = dfo.sample(n=1000, replace=False, random_state=1)\n        result = pd.concat([dfn, dfno])\n    else:\n        df2 = df[df.video_id==2]\n        dfn = df2[df.number_boxes>0]\n        dfo = df2[df.number_boxes==0]\n        dfno = dfo.sample(n=100, replace=False, random_state=1)\n        result = pd.concat([dfn, dfno])\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:13.296063Z","iopub.execute_input":"2022-02-02T17:17:13.296518Z","iopub.status.idle":"2022-02-02T17:17:13.304207Z","shell.execute_reply.started":"2022-02-02T17:17:13.296483Z","shell.execute_reply":"2022-02-02T17:17:13.303345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fold_n = 1\n# train_df= df[df.fold != fold_n]\n# val_df  = df[df.fold == fold_n]\n\ntrain_df = get_train_val(df, train=True)\nval_df = get_train_val(df, train=False)\n\n# use our dataset and defined transformations\ndataset = CotsData(train_df.reset_index(drop=True), get_transform(train=True))\ndataset_test = CotsData(val_df.reset_index(drop=True), get_transform(train=False))","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:13.305626Z","iopub.execute_input":"2022-02-02T17:17:13.306114Z","iopub.status.idle":"2022-02-02T17:17:13.338976Z","shell.execute_reply.started":"2022-02-02T17:17:13.306077Z","shell.execute_reply":"2022-02-02T17:17:13.338339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the dataset in train and test set\ntorch.manual_seed(1)\n# indices = torch.randperm(len(dataset)).tolist()\n# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=8, shuffle=True, num_workers=4,\n    collate_fn=utils.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n    collate_fn=utils.collate_fn)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:17:13.3405Z","iopub.execute_input":"2022-02-02T17:17:13.340954Z","iopub.status.idle":"2022-02-02T17:17:13.348186Z","shell.execute_reply.started":"2022-02-02T17:17:13.34092Z","shell.execute_reply":"2022-02-02T17:17:13.347422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n\n# get the model using our helper function\nmodel = get_instance_segmentation_model(num_classes)\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.05,\n                            momentum=0.9, weight_decay=0.0005)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:23:10.808052Z","iopub.execute_input":"2022-02-02T17:23:10.808664Z","iopub.status.idle":"2022-02-02T17:23:11.604331Z","shell.execute_reply.started":"2022-02-02T17:23:10.808625Z","shell.execute_reply":"2022-02-02T17:23:11.603486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# skip this cell if we only want to load the already trained model\n# let's train it for 10 epochs\n# no training\nfrom torch.optim.lr_scheduler import StepLR\nnum_epochs = 20\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, data_loader_test, device=device)\n\ntorch.save(model.state_dict(), 'checkpoint-video2.pth')","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:23:18.55019Z","iopub.execute_input":"2022-02-02T17:23:18.550787Z","iopub.status.idle":"2022-02-02T22:44:02.831141Z","shell.execute_reply.started":"2022-02-02T17:23:18.550746Z","shell.execute_reply":"2022-02-02T22:44:02.830138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PATH = 'checkpoint.pth'\n# torch.save({\n#             'epoch': epoch,\n#             'model_state_dict': model.state_dict(),\n#             'optimizer_state_dict': optimizer.state_dict(),\n#             'loss': loss,\n#             }, PATH)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:22:50.10027Z","iopub.status.idle":"2022-02-02T17:22:50.100928Z","shell.execute_reply.started":"2022-02-02T17:22:50.100653Z","shell.execute_reply":"2022-02-02T17:22:50.100698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir(r'../working')\nfrom IPython.display import FileLink\nFileLink(r'checkpoint-video2.pth')","metadata":{"execution":{"iopub.status.busy":"2022-02-02T22:49:32.669413Z","iopub.execute_input":"2022-02-02T22:49:32.669949Z","iopub.status.idle":"2022-02-02T22:49:32.679305Z","shell.execute_reply.started":"2022-02-02T22:49:32.6699Z","shell.execute_reply":"2022-02-02T22:49:32.678297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_path = '/kaggle/input/savemodel/checkpoint.pth'","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:22:50.103936Z","iopub.status.idle":"2022-02-02T17:22:50.104513Z","shell.execute_reply.started":"2022-02-02T17:22:50.104285Z","shell.execute_reply":"2022-02-02T17:22:50.104311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# state_dict = torch.load(model_path)\n# # print(state_dict.keys())\n# model.load_state_dict(state_dict)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:22:50.105796Z","iopub.status.idle":"2022-02-02T17:22:50.106423Z","shell.execute_reply.started":"2022-02-02T17:22:50.106158Z","shell.execute_reply":"2022-02-02T17:22:50.106187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def apply_nms(orig_prediction, iou_thresh=0.3, score_thresh=0.35):\n    \n#     # torchvision returns the indices of the bboxes to keep\n#     # function to implement non maximm suppression\n#     # might also need to eliminate predictions with very low scores\n#     # trim low scores first\n    \n#     keep = orig_prediction['scores'] >= score_thresh\n    \n#     scores_prediction = {}\n#     scores_prediction['boxes'] = orig_prediction['boxes'][keep]\n#     scores_prediction['scores'] = orig_prediction['scores'][keep]\n#     scores_prediction['labels'] = orig_prediction['labels'][keep]\n    \n#     keep = torchvision.ops.nms(scores_prediction['boxes'], scores_prediction['scores'], iou_thresh)\n    \n#     final_prediction = {}\n#     final_prediction['boxes'] = scores_prediction['boxes'][keep]\n#     final_prediction['scores'] = scores_prediction['scores'][keep]\n#     final_prediction['labels'] = scores_prediction['labels'][keep]\n    \n#     return final_prediction\n\n# def return_predict_string(predictions):\n#     str_p = ''\n#     for i, score in enumerate(predictions['scores']):\n#         box = predictions['boxes'][i].cpu()\n#         str_p += f'{score} {int(np.round(box[0]))} {int(np.round(box[1]))} {int(np.round(box[2]-box[0]))} {int(np.round(box[3]-box[1]))} '\n    \n#     str_p = str_p.strip(' ')\n#     if str_p == '':\n#         str_p = '0.9 716 678 54 42'\n    \n#     return str_p\n\n# def preprocess_img(img):\n#     img = img/255.\n#     x,y, c = img.shape\n#     img = img.reshape(c,x,y)\n#     return torch.from_numpy(img)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:22:50.107639Z","iopub.status.idle":"2022-02-02T17:22:50.108236Z","shell.execute_reply.started":"2022-02-02T17:22:50.107977Z","shell.execute_reply":"2022-02-02T17:22:50.108018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # pick one image from the test set\n# img, target = dataset_test[5]\n# # put the model in evaluation mode\n# model.eval()\n# with torch.no_grad():\n#     prediction = model([img.to(device=device, dtype=torch.float)])[0]\n#     final_pred = apply_nms(prediction, 0.2)\n    \n# print('predicted #boxes: ', len(prediction['labels']))\n# print('real #boxes: ', len(target['labels']))\n# print('nms predict #boxes: ', len(final_pred['labels']))\n# print('scores: ', prediction['scores'])\n# print(return_predict_string(prediction))\n\n# print(prediction)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:22:50.109489Z","iopub.status.idle":"2022-02-02T17:22:50.110177Z","shell.execute_reply.started":"2022-02-02T17:22:50.10989Z","shell.execute_reply":"2022-02-02T17:22:50.10992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import greatbarrierreef\n# rows=[]\n# ii = 0\n# env = greatbarrierreef.make_env()   # initialize the environment\n# iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\n# for (pixel_array, sample_prediction_df) in iter_test:\n#     pixel_p = preprocess_img(pixel_array)\n#     prediction = model([pixel_p.to(device, dtype=torch.float)])[0]\n#     sample_prediction_df['annotations'] = anno = '0.5 0 0 100 100' #return_predict_string(apply_nms(prediction, 0.3))  # make your predictions here\n#     rows.append([ii, anno])\n#     env.predict(sample_prediction_df)\n#     ii += 1\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:22:50.111433Z","iopub.status.idle":"2022-02-02T17:22:50.112128Z","shell.execute_reply.started":"2022-02-02T17:22:50.111835Z","shell.execute_reply":"2022-02-02T17:22:50.111865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rows\n# model([pixel_p.to(device=device, dtype=torch.float)])[0]","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:22:50.113308Z","iopub.status.idle":"2022-02-02T17:22:50.11388Z","shell.execute_reply.started":"2022-02-02T17:22:50.113617Z","shell.execute_reply":"2022-02-02T17:22:50.113642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_ds = DataAdaptor(val_df.reset_index(drop=True))","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:22:50.115068Z","iopub.status.idle":"2022-02-02T17:22:50.115703Z","shell.execute_reply.started":"2022-02-02T17:22:50.115437Z","shell.execute_reply":"2022-02-02T17:22:50.115466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# img5 = test_ds.show_image(5)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:22:50.116998Z","iopub.status.idle":"2022-02-02T17:22:50.117636Z","shell.execute_reply.started":"2022-02-02T17:22:50.117372Z","shell.execute_reply":"2022-02-02T17:22:50.117402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pixel_p*255.","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:22:50.118867Z","iopub.status.idle":"2022-02-02T17:22:50.119489Z","shell.execute_reply.started":"2022-02-02T17:22:50.119231Z","shell.execute_reply":"2022-02-02T17:22:50.11926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.imshow(pixel_p.reshape(720,1280,3))","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:22:50.120784Z","iopub.status.idle":"2022-02-02T17:22:50.121372Z","shell.execute_reply.started":"2022-02-02T17:22:50.121126Z","shell.execute_reply":"2022-02-02T17:22:50.121154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
